# Feed-Forward-Neural-Network

This project implements a general neural network with forward propagation, backpropagation for computing derivatives and the optimizer Stochastic Gradient
Descent. This implementation works with any number of layers. The code is also flexible for choice of the activation functions. 
Dataset used for this problem is MNIST classification data. The data is loaded using sklearn.

A dropout layer is also implemented to prevent overfitting.
