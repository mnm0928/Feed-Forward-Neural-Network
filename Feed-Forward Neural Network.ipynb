{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#2 Feed-Forward Neural Network."
      ],
      "metadata": {
        "id": "lTljb58M8EMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "1HQrIgh-8HLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = load_digits()"
      ],
      "metadata": {
        "id": "3zW8V1hz-Mnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing Data"
      ],
      "metadata": {
        "id": "y8SXKatLTCEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-qdNYvx-NIn",
        "outputId": "21f8a058-dd4d-4706-897d-50ac663e2683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwsvPf-o--Rj",
        "outputId": "050b7e9a-ef56-4e8c-da05-0d1987b98447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS-hLMzvHCnS",
        "outputId": "3e30e971-0494-4369-c8a1-60f400b28767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist.target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0PV8oQUHO18",
        "outputId": "135e194a-1083-446f-8ca6-e2921aa85399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1797,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(mnist.data).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "unht7P9m-QJy",
        "outputId": "44cb0268-80a2-4c13-9e87-3323b75a2567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    0    1    2     3     4     5    6    7    8    9   ...   54   55   56  \\\n",
              "0  0.0  0.0  5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "1  0.0  0.0  0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "2  0.0  0.0  0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0  ...  5.0  0.0  0.0   \n",
              "3  0.0  0.0  7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0  ...  9.0  0.0  0.0   \n",
              "4  0.0  0.0  0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
              "\n",
              "    57   58    59    60    61   62   63  \n",
              "0  0.0  6.0  13.0  10.0   0.0  0.0  0.0  \n",
              "1  0.0  0.0  11.0  16.0  10.0  0.0  0.0  \n",
              "2  0.0  0.0   3.0  11.0  16.0  9.0  0.0  \n",
              "3  0.0  7.0  13.0  13.0   9.0  0.0  0.0  \n",
              "4  0.0  0.0   2.0  16.0   4.0  0.0  0.0  \n",
              "\n",
              "[5 rows x 64 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af3e9390-df33-4f37-9f2d-a264c36b6d15\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 64 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af3e9390-df33-4f37-9f2d-a264c36b6d15')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af3e9390-df33-4f37-9f2d-a264c36b6d15 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af3e9390-df33-4f37-9f2d-a264c36b6d15');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(mnist.target).head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "py3fveDq-UG7",
        "outputId": "5e9cf638-8399-4600-c8e9-05bdecfd9a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0\n",
              "0  0\n",
              "1  1\n",
              "2  2\n",
              "3  3\n",
              "4  4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39f3500c-2c91-4bad-87e9-6447cdb481f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39f3500c-2c91-4bad-87e9-6447cdb481f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39f3500c-2c91-4bad-87e9-6447cdb481f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39f3500c-2c91-4bad-87e9-6447cdb481f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 10, figsize=(16, 6))\n",
        "for i in range(20):\n",
        "    axes[i//10, i %10].imshow(mnist.images[i], cmap='gray');\n",
        "    axes[i//10, i %10].axis('off')\n",
        "    axes[i//10, i %10].set_title(f\"target: {mnist.target[i]}\")\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "e3o0_HRU-kPd",
        "outputId": "f1e0c379-fc92-44fc-bb9c-07df43227304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x432 with 20 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION FOR NORMALIZING THE DATA\n",
        "\n",
        "def normalize_data(data):\n",
        "    normalized_data = np.zeros((data.shape))\n",
        "    for i in range(data.shape[0]):\n",
        "         img = data[i,:]\n",
        "         image = (img - np.min(img))/(np.ptp(img))\n",
        "         normalized_data[i,:] =  image\n",
        "    return normalized_data"
      ],
      "metadata": {
        "id": "HhJYwM5lQd2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FUNCTION FOR CONVERTING TARGETS INTO ONE HOT ENCODING\n",
        "\n",
        "def one_hot(y):\n",
        "    table = np.zeros((y.shape[0], 10))\n",
        "    for i in range(y.shape[0]):\n",
        "      val = y[i]\n",
        "      for row in range(10):\n",
        "        if (val==row):\n",
        "          table[i,val]=1\n",
        "    return table"
      ],
      "metadata": {
        "id": "tx3TOJBxUdhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ACTIVATION FUNCTIONS & THEIR DERIVATIVES\n",
        "\n",
        "def tanh(a):\n",
        "    return np.tanh(a)\n",
        "\n",
        "def tanh_derivative(a):\n",
        "   return (1-np.tanh(a)**2)\n",
        "\n",
        "def relu(a):\n",
        "    return np.maximum(0, a)\n",
        "\n",
        "def relu_derivative(a):\n",
        "    a[a<=0] = 0\n",
        "    a[a>0] = 1\n",
        "    return a\n",
        "\n",
        "def softmax(a):\n",
        "    e = np.exp(a-a.max())\n",
        "    output = e / np.sum(e, axis=0)\n",
        "    return output\n",
        "\n",
        "def softmax_derivative(a):\n",
        "    e = np.exp(a-a.max())\n",
        "    return e / np.sum(e, axis=0) * (1 - e / np.sum(e, axis=0))"
      ],
      "metadata": {
        "id": "IWlaVJPGnWGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#NEURAL NETWORK CLASS\n",
        "\n",
        "\"\"\"\n",
        "FUNCTIONS:\n",
        "\n",
        "initialize_parameters: initializes weights & biases\n",
        "\n",
        "fprop: forward propogation\n",
        "\n",
        "        a = w T x + b\n",
        "        z = activation_func(a)\n",
        "\n",
        "calculate_loss: cross entropy loss for multi-class classification\n",
        "\n",
        "bprop: backward propogation - to calculate gradients\n",
        "\n",
        "        if final layer:\n",
        "          delta = predicted_y - target\n",
        "        else:\n",
        "          delta = derivative_of_activation_func * weight * delta_of_next_layer\n",
        "\n",
        "      dL/dW = delta * input\n",
        "      dL/db = summation(delta)\n",
        "\n",
        "update_parameters: W = W - learning_rate * dL/dW\n",
        "                   b = b - learning_rate * dL/db\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class NeuralNetwork():\n",
        "\n",
        "    def __init__(self, num_neurons, activations_func, learning_rate, num_epochs):\n",
        "        self.num_neurons = num_neurons    #no. of neurons in each layer\n",
        "        self.activations_func = activations_func    #activation functions of each layer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_epochs\n",
        "        self.num_layers = len(self.num_neurons) - 1\n",
        "        self.parameters = dict()    #dictionary to store weights and biases\n",
        "        self.net = dict()       #dictionary to store (outputs)z values\n",
        "        self.grads = dict()       #dictionary to store gradients\n",
        "\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        print(\"Num of layers\", self.num_layers)\n",
        "        for l in range(1, self.num_layers + 1):\n",
        "            if self.activations_func[l] == 'relu': #xavier intialization method\n",
        "                self.parameters['W%s' % l] = np.random.randn(self.num_neurons[l], self.num_neurons[l-1]) / np.sqrt(self.num_neurons[l-1]/2.)\n",
        "            else:\n",
        "                self.parameters['W%s' % l] = np.random.randn(self.num_neurons[l], self.num_neurons[l-1]) / np.sqrt(self.num_neurons[l - 1])\n",
        "            self.parameters['b%s' % l] = np.zeros((self.num_neurons[l], 1))\n",
        "\n",
        "\n",
        "    def fprop(self, batch_input):\n",
        "        self.net['z0'] = batch_input\n",
        "        for l in range(1, self.num_layers + 1):\n",
        "            self.net['a%s' % l] = np.add((np.dot(self.parameters['W%s' % l], batch_input)), self.parameters['b%s' % l])\n",
        "            af = self.activations_func[l]\n",
        "            self.net['z%s' % l] = eval(af)(self.net['a%s' % l])\n",
        "            batch_input = self.net['z%s' % l]\n",
        "\n",
        "\n",
        "    def calculate_loss(self, targets):\n",
        "      y = self.net['z%s' % str(self.num_layers)]\n",
        "      loss = (-1/y.shape[1]) * np.sum(targets * np.log(y))\n",
        "      return loss\n",
        "\n",
        "    def update_parameters(self,epoch):\n",
        "       for l in range(1, self.num_layers + 1):\n",
        "           self.parameters['W%s' % l] = self.parameters['W%s' % l] - (self.learning_rate * self.grads['dw%s' % l])\n",
        "           self.parameters['b%s' % l] = self.parameters['b%s' % l] - (self.learning_rate * self.grads['db%s' % l])\n",
        "\n",
        "\n",
        "    def bprop(self, batch_target):\n",
        "        for l in range(self.num_layers, 0, -1):\n",
        "\n",
        "            if (l==self.num_layers):\n",
        "                self.grads['dz%s' % l] = self.net['z%s' % l] - batch_target\n",
        "                self.grads['dw%s' % l] = np.dot(self.grads['dz%s' % l], self.net['z%s' % str(l-1)].T)\n",
        "                self.grads['db%s' % l] = np.sum(self.grads['dz%s' % l], axis=1, keepdims=True)\n",
        "\n",
        "            else:\n",
        "                af = self.activations_func[l]\n",
        "                if (af == \"tanh\"):\n",
        "                    d = tanh_derivative(self.net['z%s' % l])\n",
        "                elif (af == \"softmax\"):\n",
        "                    d = softmax_derivative(self.net['z%s' % l])\n",
        "                elif (af == \"relu\"):\n",
        "                    d = relu_derivative(self.net['z%s' % l])\n",
        "\n",
        "\n",
        "                self.grads['da%s' % l] = np.dot(self.parameters['W%s' % str(l+1)].T, self.grads['dz%s' % str(l+1)])\n",
        "                self.grads['dz%s' % l] = np.multiply((self.grads['da%s' % l]), d)\n",
        "                self.grads['dw%s' % l] = np.dot(self.grads['dz%s' % l], self.net['z%s' % str(l-1)].T)\n",
        "                self.grads['db%s' % l] = np.sum(self.grads['dz%s' % l], axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "    def train(self, train_x, train_y):\n",
        "        train_x, train_y = shuffle(train_x, train_y, random_state=0)\n",
        "        self.initialize_parameters()\n",
        "        train_loss = []\n",
        "\n",
        "\n",
        "        for i in range(0, self.num_iterations):\n",
        "            input = train_x[i].reshape(-1,1)\n",
        "            target = train_y[i].reshape(-1,1)\n",
        "            self.fprop(input)\n",
        "            loss = self.calculate_loss(target)\n",
        "            self.bprop(target)\n",
        "            self.update_parameters(i)\n",
        "\n",
        "            train_loss.append(loss)\n",
        "            print(\"Epoch %i: training loss %f \" % (i, loss))\n",
        "\n",
        "    def test(self, test_x, test_t):\n",
        "        self.total_acc = 0\n",
        "        count = 0\n",
        "        predicted = []\n",
        "        true = []\n",
        "        for i in range(test_x.shape[0]):\n",
        "            test_xi = test_x[i]\n",
        "            test_ti = test_t[i]\n",
        "            test_xi = test_xi.reshape(-1,1)\n",
        "            test_ti = test_ti.reshape(-1, 1)\n",
        "            self.fprop(test_xi)\n",
        "            output = self.net['z%s'% str(self.num_layers)]\n",
        "            if np.argmax(output) == np.argmax(test_ti):\n",
        "                self.total_acc += 1\n",
        "            predicted.append(np.argmax(output))\n",
        "            true.append(np.argmax(test_ti))\n",
        "            count += 1\n",
        "\n",
        "        acc = self.total_acc / count * 100\n",
        "\n",
        "        return acc"
      ],
      "metadata": {
        "id": "Qhmt2wQgnvJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting data into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size = 0.2, stratify=mnist.target)"
      ],
      "metadata": {
        "id": "1-6xI2MKPy2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalizing data\n",
        "X_train = normalize_data(X_train)\n",
        "X_test = normalize_data(X_test)"
      ],
      "metadata": {
        "id": "3cfyKqDxS8oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting targets into one hot encoding\n",
        "y_train = one_hot(y_train)\n",
        "y_test = one_hot(y_test)"
      ],
      "metadata": {
        "id": "-pWlFUXJVavZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create l-dim network by just adding num of neurons in layer_dim\n",
        "# first and last elements represent input and output layers dim\n",
        "layer_dim = [64, 128, 64, 10]\n",
        "\n",
        "# add activation functions name here.\n",
        "# input layer activation function is None\n",
        "activations = ['None', 'tanh', 'relu', 'softmax']\n",
        "assert len(layer_dim) ==  len(activations), \"layer dim or activation is missing..\"\n",
        "\n",
        "# hyper parameters of neural network\n",
        "learning_rate = 1e-2\n",
        "num_epochs = 1000\n",
        "\n",
        "\n",
        "nn = NeuralNetwork(layer_dim, activations, learning_rate, num_epochs)\n",
        "\n",
        "# train neural network\n",
        "nn.train(X_train, y_train)\n",
        "\n",
        "\n",
        "# test neural network\n",
        "train_acc = nn.test(X_train, y_train)\n",
        "print(\"training acc..\", np.round(train_acc, 4))\n",
        "test_acc = nn.test(X_test, y_test)\n",
        "print(\"testing loss..\", np.round(test_acc, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srPGIdKSy-Ch",
        "outputId": "d7d166c7-ad31-416b-ef57-5bd4af1b5626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of layers 3\n",
            "Epoch 0: training loss 2.368613 \n",
            "Epoch 1: training loss 2.465599 \n",
            "Epoch 2: training loss 2.151649 \n",
            "Epoch 3: training loss 2.801175 \n",
            "Epoch 4: training loss 2.335159 \n",
            "Epoch 5: training loss 3.365377 \n",
            "Epoch 6: training loss 2.328801 \n",
            "Epoch 7: training loss 2.174423 \n",
            "Epoch 8: training loss 2.429679 \n",
            "Epoch 9: training loss 2.834343 \n",
            "Epoch 10: training loss 2.223816 \n",
            "Epoch 11: training loss 1.912923 \n",
            "Epoch 12: training loss 1.493433 \n",
            "Epoch 13: training loss 2.055284 \n",
            "Epoch 14: training loss 2.478412 \n",
            "Epoch 15: training loss 2.191578 \n",
            "Epoch 16: training loss 2.609239 \n",
            "Epoch 17: training loss 2.876098 \n",
            "Epoch 18: training loss 2.386874 \n",
            "Epoch 19: training loss 1.796478 \n",
            "Epoch 20: training loss 2.731506 \n",
            "Epoch 21: training loss 2.544227 \n",
            "Epoch 22: training loss 2.913544 \n",
            "Epoch 23: training loss 1.930484 \n",
            "Epoch 24: training loss 2.159371 \n",
            "Epoch 25: training loss 2.452373 \n",
            "Epoch 26: training loss 2.006428 \n",
            "Epoch 27: training loss 2.295092 \n",
            "Epoch 28: training loss 2.300569 \n",
            "Epoch 29: training loss 2.300385 \n",
            "Epoch 30: training loss 2.288231 \n",
            "Epoch 31: training loss 2.477763 \n",
            "Epoch 32: training loss 2.378498 \n",
            "Epoch 33: training loss 1.804074 \n",
            "Epoch 34: training loss 2.772572 \n",
            "Epoch 35: training loss 2.203575 \n",
            "Epoch 36: training loss 2.260983 \n",
            "Epoch 37: training loss 2.371127 \n",
            "Epoch 38: training loss 2.518340 \n",
            "Epoch 39: training loss 1.572664 \n",
            "Epoch 40: training loss 2.346132 \n",
            "Epoch 41: training loss 1.849131 \n",
            "Epoch 42: training loss 2.058784 \n",
            "Epoch 43: training loss 1.915069 \n",
            "Epoch 44: training loss 2.394157 \n",
            "Epoch 45: training loss 2.398844 \n",
            "Epoch 46: training loss 2.029512 \n",
            "Epoch 47: training loss 2.707029 \n",
            "Epoch 48: training loss 2.433494 \n",
            "Epoch 49: training loss 1.934066 \n",
            "Epoch 50: training loss 2.279283 \n",
            "Epoch 51: training loss 2.356501 \n",
            "Epoch 52: training loss 1.561372 \n",
            "Epoch 53: training loss 2.228315 \n",
            "Epoch 54: training loss 2.117695 \n",
            "Epoch 55: training loss 2.769187 \n",
            "Epoch 56: training loss 2.238036 \n",
            "Epoch 57: training loss 1.932270 \n",
            "Epoch 58: training loss 2.357237 \n",
            "Epoch 59: training loss 2.389555 \n",
            "Epoch 60: training loss 2.036502 \n",
            "Epoch 61: training loss 1.932051 \n",
            "Epoch 62: training loss 1.609674 \n",
            "Epoch 63: training loss 1.941402 \n",
            "Epoch 64: training loss 2.921251 \n",
            "Epoch 65: training loss 1.644466 \n",
            "Epoch 66: training loss 2.014577 \n",
            "Epoch 67: training loss 1.372175 \n",
            "Epoch 68: training loss 1.436107 \n",
            "Epoch 69: training loss 2.163569 \n",
            "Epoch 70: training loss 2.403703 \n",
            "Epoch 71: training loss 2.503500 \n",
            "Epoch 72: training loss 1.523938 \n",
            "Epoch 73: training loss 2.664891 \n",
            "Epoch 74: training loss 2.584461 \n",
            "Epoch 75: training loss 2.339721 \n",
            "Epoch 76: training loss 2.485920 \n",
            "Epoch 77: training loss 1.355143 \n",
            "Epoch 78: training loss 2.264822 \n",
            "Epoch 79: training loss 2.336461 \n",
            "Epoch 80: training loss 2.506607 \n",
            "Epoch 81: training loss 2.430007 \n",
            "Epoch 82: training loss 2.567844 \n",
            "Epoch 83: training loss 1.664789 \n",
            "Epoch 84: training loss 2.432979 \n",
            "Epoch 85: training loss 2.201843 \n",
            "Epoch 86: training loss 2.121181 \n",
            "Epoch 87: training loss 1.963618 \n",
            "Epoch 88: training loss 2.040462 \n",
            "Epoch 89: training loss 2.407969 \n",
            "Epoch 90: training loss 2.141566 \n",
            "Epoch 91: training loss 1.676474 \n",
            "Epoch 92: training loss 1.893664 \n",
            "Epoch 93: training loss 1.905393 \n",
            "Epoch 94: training loss 1.978196 \n",
            "Epoch 95: training loss 2.244538 \n",
            "Epoch 96: training loss 2.289403 \n",
            "Epoch 97: training loss 1.722455 \n",
            "Epoch 98: training loss 2.624209 \n",
            "Epoch 99: training loss 2.063348 \n",
            "Epoch 100: training loss 1.489668 \n",
            "Epoch 101: training loss 2.098617 \n",
            "Epoch 102: training loss 2.334649 \n",
            "Epoch 103: training loss 1.780150 \n",
            "Epoch 104: training loss 2.042806 \n",
            "Epoch 105: training loss 2.118691 \n",
            "Epoch 106: training loss 2.065795 \n",
            "Epoch 107: training loss 2.421909 \n",
            "Epoch 108: training loss 1.994246 \n",
            "Epoch 109: training loss 1.517992 \n",
            "Epoch 110: training loss 1.994633 \n",
            "Epoch 111: training loss 1.833783 \n",
            "Epoch 112: training loss 1.367045 \n",
            "Epoch 113: training loss 1.670862 \n",
            "Epoch 114: training loss 2.123335 \n",
            "Epoch 115: training loss 1.473334 \n",
            "Epoch 116: training loss 1.321656 \n",
            "Epoch 117: training loss 2.490771 \n",
            "Epoch 118: training loss 2.042470 \n",
            "Epoch 119: training loss 1.905385 \n",
            "Epoch 120: training loss 2.668602 \n",
            "Epoch 121: training loss 1.350843 \n",
            "Epoch 122: training loss 1.564055 \n",
            "Epoch 123: training loss 2.559007 \n",
            "Epoch 124: training loss 2.453809 \n",
            "Epoch 125: training loss 1.785049 \n",
            "Epoch 126: training loss 1.419674 \n",
            "Epoch 127: training loss 2.602273 \n",
            "Epoch 128: training loss 2.170271 \n",
            "Epoch 129: training loss 1.688124 \n",
            "Epoch 130: training loss 1.341080 \n",
            "Epoch 131: training loss 1.656939 \n",
            "Epoch 132: training loss 1.928308 \n",
            "Epoch 133: training loss 2.489502 \n",
            "Epoch 134: training loss 1.232388 \n",
            "Epoch 135: training loss 1.903440 \n",
            "Epoch 136: training loss 1.992181 \n",
            "Epoch 137: training loss 2.548881 \n",
            "Epoch 138: training loss 1.474305 \n",
            "Epoch 139: training loss 2.175103 \n",
            "Epoch 140: training loss 1.786355 \n",
            "Epoch 141: training loss 2.426906 \n",
            "Epoch 142: training loss 1.987570 \n",
            "Epoch 143: training loss 2.246748 \n",
            "Epoch 144: training loss 1.624137 \n",
            "Epoch 145: training loss 1.703918 \n",
            "Epoch 146: training loss 1.555914 \n",
            "Epoch 147: training loss 1.395079 \n",
            "Epoch 148: training loss 1.578123 \n",
            "Epoch 149: training loss 1.914017 \n",
            "Epoch 150: training loss 1.498356 \n",
            "Epoch 151: training loss 1.635479 \n",
            "Epoch 152: training loss 1.697962 \n",
            "Epoch 153: training loss 1.680117 \n",
            "Epoch 154: training loss 1.921730 \n",
            "Epoch 155: training loss 1.565930 \n",
            "Epoch 156: training loss 0.971990 \n",
            "Epoch 157: training loss 2.336510 \n",
            "Epoch 158: training loss 1.155126 \n",
            "Epoch 159: training loss 1.548326 \n",
            "Epoch 160: training loss 2.197702 \n",
            "Epoch 161: training loss 1.680255 \n",
            "Epoch 162: training loss 2.307810 \n",
            "Epoch 163: training loss 1.694754 \n",
            "Epoch 164: training loss 2.151576 \n",
            "Epoch 165: training loss 1.940315 \n",
            "Epoch 166: training loss 1.648399 \n",
            "Epoch 167: training loss 2.359671 \n",
            "Epoch 168: training loss 1.640805 \n",
            "Epoch 169: training loss 0.958125 \n",
            "Epoch 170: training loss 2.096199 \n",
            "Epoch 171: training loss 1.393863 \n",
            "Epoch 172: training loss 1.170510 \n",
            "Epoch 173: training loss 2.293664 \n",
            "Epoch 174: training loss 0.936771 \n",
            "Epoch 175: training loss 1.689183 \n",
            "Epoch 176: training loss 2.681811 \n",
            "Epoch 177: training loss 0.966420 \n",
            "Epoch 178: training loss 1.443252 \n",
            "Epoch 179: training loss 2.073640 \n",
            "Epoch 180: training loss 1.776861 \n",
            "Epoch 181: training loss 2.584420 \n",
            "Epoch 182: training loss 1.945047 \n",
            "Epoch 183: training loss 1.160914 \n",
            "Epoch 184: training loss 1.331335 \n",
            "Epoch 185: training loss 1.562687 \n",
            "Epoch 186: training loss 1.499556 \n",
            "Epoch 187: training loss 1.713564 \n",
            "Epoch 188: training loss 1.265145 \n",
            "Epoch 189: training loss 1.330264 \n",
            "Epoch 190: training loss 2.433965 \n",
            "Epoch 191: training loss 2.099765 \n",
            "Epoch 192: training loss 2.130821 \n",
            "Epoch 193: training loss 1.627223 \n",
            "Epoch 194: training loss 1.319780 \n",
            "Epoch 195: training loss 1.750340 \n",
            "Epoch 196: training loss 2.223024 \n",
            "Epoch 197: training loss 0.975647 \n",
            "Epoch 198: training loss 1.086342 \n",
            "Epoch 199: training loss 1.426968 \n",
            "Epoch 200: training loss 0.999647 \n",
            "Epoch 201: training loss 1.223619 \n",
            "Epoch 202: training loss 1.910549 \n",
            "Epoch 203: training loss 1.881439 \n",
            "Epoch 204: training loss 1.841139 \n",
            "Epoch 205: training loss 1.394141 \n",
            "Epoch 206: training loss 1.202756 \n",
            "Epoch 207: training loss 1.622859 \n",
            "Epoch 208: training loss 1.682276 \n",
            "Epoch 209: training loss 1.988294 \n",
            "Epoch 210: training loss 0.491480 \n",
            "Epoch 211: training loss 2.216239 \n",
            "Epoch 212: training loss 1.728934 \n",
            "Epoch 213: training loss 1.855036 \n",
            "Epoch 214: training loss 1.069490 \n",
            "Epoch 215: training loss 0.637686 \n",
            "Epoch 216: training loss 0.861253 \n",
            "Epoch 217: training loss 1.766640 \n",
            "Epoch 218: training loss 1.598982 \n",
            "Epoch 219: training loss 0.286237 \n",
            "Epoch 220: training loss 1.356704 \n",
            "Epoch 221: training loss 2.641483 \n",
            "Epoch 222: training loss 1.382412 \n",
            "Epoch 223: training loss 3.012707 \n",
            "Epoch 224: training loss 1.938099 \n",
            "Epoch 225: training loss 0.416943 \n",
            "Epoch 226: training loss 1.282244 \n",
            "Epoch 227: training loss 0.784973 \n",
            "Epoch 228: training loss 0.276156 \n",
            "Epoch 229: training loss 1.565905 \n",
            "Epoch 230: training loss 1.905648 \n",
            "Epoch 231: training loss 1.502752 \n",
            "Epoch 232: training loss 1.174106 \n",
            "Epoch 233: training loss 1.117105 \n",
            "Epoch 234: training loss 2.111875 \n",
            "Epoch 235: training loss 0.503179 \n",
            "Epoch 236: training loss 2.115465 \n",
            "Epoch 237: training loss 1.600071 \n",
            "Epoch 238: training loss 1.587847 \n",
            "Epoch 239: training loss 1.534476 \n",
            "Epoch 240: training loss 0.916330 \n",
            "Epoch 241: training loss 1.271836 \n",
            "Epoch 242: training loss 1.364630 \n",
            "Epoch 243: training loss 1.415088 \n",
            "Epoch 244: training loss 1.437033 \n",
            "Epoch 245: training loss 1.640426 \n",
            "Epoch 246: training loss 0.800968 \n",
            "Epoch 247: training loss 1.401689 \n",
            "Epoch 248: training loss 1.680065 \n",
            "Epoch 249: training loss 1.032579 \n",
            "Epoch 250: training loss 1.711171 \n",
            "Epoch 251: training loss 2.459061 \n",
            "Epoch 252: training loss 0.658260 \n",
            "Epoch 253: training loss 1.371468 \n",
            "Epoch 254: training loss 2.517087 \n",
            "Epoch 255: training loss 2.549725 \n",
            "Epoch 256: training loss 0.735331 \n",
            "Epoch 257: training loss 0.919678 \n",
            "Epoch 258: training loss 2.221532 \n",
            "Epoch 259: training loss 1.267511 \n",
            "Epoch 260: training loss 2.421678 \n",
            "Epoch 261: training loss 1.141437 \n",
            "Epoch 262: training loss 1.602496 \n",
            "Epoch 263: training loss 0.686664 \n",
            "Epoch 264: training loss 1.234311 \n",
            "Epoch 265: training loss 1.833018 \n",
            "Epoch 266: training loss 1.240742 \n",
            "Epoch 267: training loss 2.191449 \n",
            "Epoch 268: training loss 1.173406 \n",
            "Epoch 269: training loss 1.003425 \n",
            "Epoch 270: training loss 1.639254 \n",
            "Epoch 271: training loss 1.140763 \n",
            "Epoch 272: training loss 2.059793 \n",
            "Epoch 273: training loss 1.106665 \n",
            "Epoch 274: training loss 1.168288 \n",
            "Epoch 275: training loss 1.840643 \n",
            "Epoch 276: training loss 0.916464 \n",
            "Epoch 277: training loss 1.013686 \n",
            "Epoch 278: training loss 1.360431 \n",
            "Epoch 279: training loss 2.918976 \n",
            "Epoch 280: training loss 2.062166 \n",
            "Epoch 281: training loss 1.045310 \n",
            "Epoch 282: training loss 1.542116 \n",
            "Epoch 283: training loss 0.994054 \n",
            "Epoch 284: training loss 1.846838 \n",
            "Epoch 285: training loss 1.985950 \n",
            "Epoch 286: training loss 0.640592 \n",
            "Epoch 287: training loss 3.085304 \n",
            "Epoch 288: training loss 1.612787 \n",
            "Epoch 289: training loss 1.114256 \n",
            "Epoch 290: training loss 1.198878 \n",
            "Epoch 291: training loss 0.764326 \n",
            "Epoch 292: training loss 0.993329 \n",
            "Epoch 293: training loss 1.103994 \n",
            "Epoch 294: training loss 1.735650 \n",
            "Epoch 295: training loss 1.323530 \n",
            "Epoch 296: training loss 1.020926 \n",
            "Epoch 297: training loss 1.140588 \n",
            "Epoch 298: training loss 1.501175 \n",
            "Epoch 299: training loss 1.339224 \n",
            "Epoch 300: training loss 2.040233 \n",
            "Epoch 301: training loss 0.995528 \n",
            "Epoch 302: training loss 0.812029 \n",
            "Epoch 303: training loss 0.792344 \n",
            "Epoch 304: training loss 1.060790 \n",
            "Epoch 305: training loss 1.018660 \n",
            "Epoch 306: training loss 0.851043 \n",
            "Epoch 307: training loss 0.691790 \n",
            "Epoch 308: training loss 0.627524 \n",
            "Epoch 309: training loss 1.130716 \n",
            "Epoch 310: training loss 1.363514 \n",
            "Epoch 311: training loss 1.961725 \n",
            "Epoch 312: training loss 0.337570 \n",
            "Epoch 313: training loss 0.542045 \n",
            "Epoch 314: training loss 0.605535 \n",
            "Epoch 315: training loss 1.982919 \n",
            "Epoch 316: training loss 0.466120 \n",
            "Epoch 317: training loss 2.301815 \n",
            "Epoch 318: training loss 1.506659 \n",
            "Epoch 319: training loss 1.817169 \n",
            "Epoch 320: training loss 1.508181 \n",
            "Epoch 321: training loss 2.193813 \n",
            "Epoch 322: training loss 1.459552 \n",
            "Epoch 323: training loss 1.370954 \n",
            "Epoch 324: training loss 0.982723 \n",
            "Epoch 325: training loss 1.533827 \n",
            "Epoch 326: training loss 0.628944 \n",
            "Epoch 327: training loss 0.436259 \n",
            "Epoch 328: training loss 1.854841 \n",
            "Epoch 329: training loss 0.796673 \n",
            "Epoch 330: training loss 0.910247 \n",
            "Epoch 331: training loss 1.749628 \n",
            "Epoch 332: training loss 1.138373 \n",
            "Epoch 333: training loss 1.047306 \n",
            "Epoch 334: training loss 0.470928 \n",
            "Epoch 335: training loss 0.635605 \n",
            "Epoch 336: training loss 0.968495 \n",
            "Epoch 337: training loss 0.352002 \n",
            "Epoch 338: training loss 2.412214 \n",
            "Epoch 339: training loss 0.749595 \n",
            "Epoch 340: training loss 1.561448 \n",
            "Epoch 341: training loss 1.162770 \n",
            "Epoch 342: training loss 1.765885 \n",
            "Epoch 343: training loss 2.246617 \n",
            "Epoch 344: training loss 0.922370 \n",
            "Epoch 345: training loss 0.884607 \n",
            "Epoch 346: training loss 1.295248 \n",
            "Epoch 347: training loss 1.307958 \n",
            "Epoch 348: training loss 2.092552 \n",
            "Epoch 349: training loss 0.738541 \n",
            "Epoch 350: training loss 0.795877 \n",
            "Epoch 351: training loss 3.010606 \n",
            "Epoch 352: training loss 1.119186 \n",
            "Epoch 353: training loss 0.802358 \n",
            "Epoch 354: training loss 1.344356 \n",
            "Epoch 355: training loss 0.543966 \n",
            "Epoch 356: training loss 0.300107 \n",
            "Epoch 357: training loss 2.420886 \n",
            "Epoch 358: training loss 0.663982 \n",
            "Epoch 359: training loss 2.379781 \n",
            "Epoch 360: training loss 0.323279 \n",
            "Epoch 361: training loss 2.121171 \n",
            "Epoch 362: training loss 1.075660 \n",
            "Epoch 363: training loss 1.670640 \n",
            "Epoch 364: training loss 1.065150 \n",
            "Epoch 365: training loss 1.052611 \n",
            "Epoch 366: training loss 1.348315 \n",
            "Epoch 367: training loss 1.226628 \n",
            "Epoch 368: training loss 0.740152 \n",
            "Epoch 369: training loss 1.360818 \n",
            "Epoch 370: training loss 0.453416 \n",
            "Epoch 371: training loss 2.011366 \n",
            "Epoch 372: training loss 2.592795 \n",
            "Epoch 373: training loss 0.945760 \n",
            "Epoch 374: training loss 1.293785 \n",
            "Epoch 375: training loss 1.014905 \n",
            "Epoch 376: training loss 2.543604 \n",
            "Epoch 377: training loss 1.645062 \n",
            "Epoch 378: training loss 1.411947 \n",
            "Epoch 379: training loss 1.432067 \n",
            "Epoch 380: training loss 1.014212 \n",
            "Epoch 381: training loss 1.680342 \n",
            "Epoch 382: training loss 1.377286 \n",
            "Epoch 383: training loss 1.580670 \n",
            "Epoch 384: training loss 1.175409 \n",
            "Epoch 385: training loss 0.617524 \n",
            "Epoch 386: training loss 1.321729 \n",
            "Epoch 387: training loss 0.858337 \n",
            "Epoch 388: training loss 0.634064 \n",
            "Epoch 389: training loss 0.951400 \n",
            "Epoch 390: training loss 0.780811 \n",
            "Epoch 391: training loss 1.806340 \n",
            "Epoch 392: training loss 1.108034 \n",
            "Epoch 393: training loss 1.169257 \n",
            "Epoch 394: training loss 1.120641 \n",
            "Epoch 395: training loss 0.403397 \n",
            "Epoch 396: training loss 0.949199 \n",
            "Epoch 397: training loss 0.574057 \n",
            "Epoch 398: training loss 1.033583 \n",
            "Epoch 399: training loss 0.962044 \n",
            "Epoch 400: training loss 0.480967 \n",
            "Epoch 401: training loss 0.430704 \n",
            "Epoch 402: training loss 0.538654 \n",
            "Epoch 403: training loss 1.301331 \n",
            "Epoch 404: training loss 0.858810 \n",
            "Epoch 405: training loss 0.718399 \n",
            "Epoch 406: training loss 1.098353 \n",
            "Epoch 407: training loss 1.156203 \n",
            "Epoch 408: training loss 1.199734 \n",
            "Epoch 409: training loss 1.062725 \n",
            "Epoch 410: training loss 1.930651 \n",
            "Epoch 411: training loss 0.731758 \n",
            "Epoch 412: training loss 1.630243 \n",
            "Epoch 413: training loss 0.394347 \n",
            "Epoch 414: training loss 0.877158 \n",
            "Epoch 415: training loss 0.499926 \n",
            "Epoch 416: training loss 0.526373 \n",
            "Epoch 417: training loss 0.964647 \n",
            "Epoch 418: training loss 0.913312 \n",
            "Epoch 419: training loss 1.466897 \n",
            "Epoch 420: training loss 0.410276 \n",
            "Epoch 421: training loss 1.753145 \n",
            "Epoch 422: training loss 0.946282 \n",
            "Epoch 423: training loss 0.384579 \n",
            "Epoch 424: training loss 0.841961 \n",
            "Epoch 425: training loss 1.238381 \n",
            "Epoch 426: training loss 0.424138 \n",
            "Epoch 427: training loss 0.314047 \n",
            "Epoch 428: training loss 0.706745 \n",
            "Epoch 429: training loss 0.699365 \n",
            "Epoch 430: training loss 0.429279 \n",
            "Epoch 431: training loss 2.516968 \n",
            "Epoch 432: training loss 1.144406 \n",
            "Epoch 433: training loss 1.464531 \n",
            "Epoch 434: training loss 0.939642 \n",
            "Epoch 435: training loss 0.464720 \n",
            "Epoch 436: training loss 0.558057 \n",
            "Epoch 437: training loss 1.856684 \n",
            "Epoch 438: training loss 0.217832 \n",
            "Epoch 439: training loss 0.430561 \n",
            "Epoch 440: training loss 1.513591 \n",
            "Epoch 441: training loss 1.869994 \n",
            "Epoch 442: training loss 0.451352 \n",
            "Epoch 443: training loss 0.436257 \n",
            "Epoch 444: training loss 0.929403 \n",
            "Epoch 445: training loss 2.903256 \n",
            "Epoch 446: training loss 0.873057 \n",
            "Epoch 447: training loss 0.635322 \n",
            "Epoch 448: training loss 0.609285 \n",
            "Epoch 449: training loss 0.514058 \n",
            "Epoch 450: training loss 0.339197 \n",
            "Epoch 451: training loss 0.508699 \n",
            "Epoch 452: training loss 0.994971 \n",
            "Epoch 453: training loss 1.884273 \n",
            "Epoch 454: training loss 0.337854 \n",
            "Epoch 455: training loss 0.766597 \n",
            "Epoch 456: training loss 0.576099 \n",
            "Epoch 457: training loss 0.398019 \n",
            "Epoch 458: training loss 1.707094 \n",
            "Epoch 459: training loss 0.633642 \n",
            "Epoch 460: training loss 0.546317 \n",
            "Epoch 461: training loss 0.455901 \n",
            "Epoch 462: training loss 0.572809 \n",
            "Epoch 463: training loss 1.580231 \n",
            "Epoch 464: training loss 0.458986 \n",
            "Epoch 465: training loss 0.499906 \n",
            "Epoch 466: training loss 0.243474 \n",
            "Epoch 467: training loss 0.598733 \n",
            "Epoch 468: training loss 0.443451 \n",
            "Epoch 469: training loss 0.594025 \n",
            "Epoch 470: training loss 0.930860 \n",
            "Epoch 471: training loss 0.548307 \n",
            "Epoch 472: training loss 0.545682 \n",
            "Epoch 473: training loss 2.405358 \n",
            "Epoch 474: training loss 0.492499 \n",
            "Epoch 475: training loss 0.445214 \n",
            "Epoch 476: training loss 0.258175 \n",
            "Epoch 477: training loss 1.272182 \n",
            "Epoch 478: training loss 0.349401 \n",
            "Epoch 479: training loss 0.485018 \n",
            "Epoch 480: training loss 1.075719 \n",
            "Epoch 481: training loss 0.328307 \n",
            "Epoch 482: training loss 0.488533 \n",
            "Epoch 483: training loss 0.923631 \n",
            "Epoch 484: training loss 0.262409 \n",
            "Epoch 485: training loss 1.285103 \n",
            "Epoch 486: training loss 0.578888 \n",
            "Epoch 487: training loss 0.801266 \n",
            "Epoch 488: training loss 0.300313 \n",
            "Epoch 489: training loss 0.182265 \n",
            "Epoch 490: training loss 1.140444 \n",
            "Epoch 491: training loss 0.415141 \n",
            "Epoch 492: training loss 1.887760 \n",
            "Epoch 493: training loss 0.877143 \n",
            "Epoch 494: training loss 1.854991 \n",
            "Epoch 495: training loss 0.601605 \n",
            "Epoch 496: training loss 1.428913 \n",
            "Epoch 497: training loss 0.604965 \n",
            "Epoch 498: training loss 1.272252 \n",
            "Epoch 499: training loss 0.497671 \n",
            "Epoch 500: training loss 0.194628 \n",
            "Epoch 501: training loss 0.136928 \n",
            "Epoch 502: training loss 1.910560 \n",
            "Epoch 503: training loss 0.191375 \n",
            "Epoch 504: training loss 0.446888 \n",
            "Epoch 505: training loss 1.722738 \n",
            "Epoch 506: training loss 0.353517 \n",
            "Epoch 507: training loss 1.292892 \n",
            "Epoch 508: training loss 0.296663 \n",
            "Epoch 509: training loss 1.440675 \n",
            "Epoch 510: training loss 2.001140 \n",
            "Epoch 511: training loss 0.848037 \n",
            "Epoch 512: training loss 0.157533 \n",
            "Epoch 513: training loss 0.501942 \n",
            "Epoch 514: training loss 1.617587 \n",
            "Epoch 515: training loss 1.155882 \n",
            "Epoch 516: training loss 0.816743 \n",
            "Epoch 517: training loss 1.067249 \n",
            "Epoch 518: training loss 0.736007 \n",
            "Epoch 519: training loss 0.263215 \n",
            "Epoch 520: training loss 0.657413 \n",
            "Epoch 521: training loss 0.716944 \n",
            "Epoch 522: training loss 0.712738 \n",
            "Epoch 523: training loss 0.281431 \n",
            "Epoch 524: training loss 0.663930 \n",
            "Epoch 525: training loss 0.389419 \n",
            "Epoch 526: training loss 0.095222 \n",
            "Epoch 527: training loss 0.538533 \n",
            "Epoch 528: training loss 1.584797 \n",
            "Epoch 529: training loss 1.567815 \n",
            "Epoch 530: training loss 0.698909 \n",
            "Epoch 531: training loss 1.522215 \n",
            "Epoch 532: training loss 0.222340 \n",
            "Epoch 533: training loss 0.216607 \n",
            "Epoch 534: training loss 0.275636 \n",
            "Epoch 535: training loss 0.880392 \n",
            "Epoch 536: training loss 0.879103 \n",
            "Epoch 537: training loss 0.331084 \n",
            "Epoch 538: training loss 1.219077 \n",
            "Epoch 539: training loss 0.488196 \n",
            "Epoch 540: training loss 2.431787 \n",
            "Epoch 541: training loss 0.919380 \n",
            "Epoch 542: training loss 1.471858 \n",
            "Epoch 543: training loss 1.098472 \n",
            "Epoch 544: training loss 0.134283 \n",
            "Epoch 545: training loss 0.599056 \n",
            "Epoch 546: training loss 0.438020 \n",
            "Epoch 547: training loss 2.565604 \n",
            "Epoch 548: training loss 0.257555 \n",
            "Epoch 549: training loss 0.327822 \n",
            "Epoch 550: training loss 0.543248 \n",
            "Epoch 551: training loss 0.229574 \n",
            "Epoch 552: training loss 1.978283 \n",
            "Epoch 553: training loss 1.395998 \n",
            "Epoch 554: training loss 0.282610 \n",
            "Epoch 555: training loss 0.943362 \n",
            "Epoch 556: training loss 0.474821 \n",
            "Epoch 557: training loss 0.863801 \n",
            "Epoch 558: training loss 0.932716 \n",
            "Epoch 559: training loss 0.197385 \n",
            "Epoch 560: training loss 0.438757 \n",
            "Epoch 561: training loss 0.576370 \n",
            "Epoch 562: training loss 0.532009 \n",
            "Epoch 563: training loss 0.161118 \n",
            "Epoch 564: training loss 0.991863 \n",
            "Epoch 565: training loss 0.785883 \n",
            "Epoch 566: training loss 0.426641 \n",
            "Epoch 567: training loss 0.694226 \n",
            "Epoch 568: training loss 0.665315 \n",
            "Epoch 569: training loss 0.412317 \n",
            "Epoch 570: training loss 0.303512 \n",
            "Epoch 571: training loss 0.703806 \n",
            "Epoch 572: training loss 0.377635 \n",
            "Epoch 573: training loss 0.710952 \n",
            "Epoch 574: training loss 0.069504 \n",
            "Epoch 575: training loss 0.201610 \n",
            "Epoch 576: training loss 0.786033 \n",
            "Epoch 577: training loss 0.693497 \n",
            "Epoch 578: training loss 0.242529 \n",
            "Epoch 579: training loss 0.610135 \n",
            "Epoch 580: training loss 0.559595 \n",
            "Epoch 581: training loss 0.330852 \n",
            "Epoch 582: training loss 0.306649 \n",
            "Epoch 583: training loss 1.818818 \n",
            "Epoch 584: training loss 2.564791 \n",
            "Epoch 585: training loss 0.711018 \n",
            "Epoch 586: training loss 1.107061 \n",
            "Epoch 587: training loss 0.160620 \n",
            "Epoch 588: training loss 0.918724 \n",
            "Epoch 589: training loss 0.091999 \n",
            "Epoch 590: training loss 0.151772 \n",
            "Epoch 591: training loss 0.328414 \n",
            "Epoch 592: training loss 0.563827 \n",
            "Epoch 593: training loss 0.326218 \n",
            "Epoch 594: training loss 0.424457 \n",
            "Epoch 595: training loss 0.146094 \n",
            "Epoch 596: training loss 0.763027 \n",
            "Epoch 597: training loss 0.086279 \n",
            "Epoch 598: training loss 1.821966 \n",
            "Epoch 599: training loss 0.229611 \n",
            "Epoch 600: training loss 0.507932 \n",
            "Epoch 601: training loss 0.522812 \n",
            "Epoch 602: training loss 0.791180 \n",
            "Epoch 603: training loss 0.554869 \n",
            "Epoch 604: training loss 0.824435 \n",
            "Epoch 605: training loss 0.367412 \n",
            "Epoch 606: training loss 0.760772 \n",
            "Epoch 607: training loss 0.429725 \n",
            "Epoch 608: training loss 1.727896 \n",
            "Epoch 609: training loss 0.252167 \n",
            "Epoch 610: training loss 0.395005 \n",
            "Epoch 611: training loss 0.060167 \n",
            "Epoch 612: training loss 0.139851 \n",
            "Epoch 613: training loss 0.166514 \n",
            "Epoch 614: training loss 0.712558 \n",
            "Epoch 615: training loss 0.277511 \n",
            "Epoch 616: training loss 0.637891 \n",
            "Epoch 617: training loss 0.367926 \n",
            "Epoch 618: training loss 0.071894 \n",
            "Epoch 619: training loss 0.099077 \n",
            "Epoch 620: training loss 2.777839 \n",
            "Epoch 621: training loss 0.391618 \n",
            "Epoch 622: training loss 0.595581 \n",
            "Epoch 623: training loss 0.630015 \n",
            "Epoch 624: training loss 0.347334 \n",
            "Epoch 625: training loss 0.557088 \n",
            "Epoch 626: training loss 0.904706 \n",
            "Epoch 627: training loss 1.123704 \n",
            "Epoch 628: training loss 0.349147 \n",
            "Epoch 629: training loss 0.051062 \n",
            "Epoch 630: training loss 0.252879 \n",
            "Epoch 631: training loss 1.215874 \n",
            "Epoch 632: training loss 0.427116 \n",
            "Epoch 633: training loss 0.098364 \n",
            "Epoch 634: training loss 0.147083 \n",
            "Epoch 635: training loss 0.324339 \n",
            "Epoch 636: training loss 1.043067 \n",
            "Epoch 637: training loss 0.972503 \n",
            "Epoch 638: training loss 1.280655 \n",
            "Epoch 639: training loss 0.557909 \n",
            "Epoch 640: training loss 0.282960 \n",
            "Epoch 641: training loss 1.150306 \n",
            "Epoch 642: training loss 1.200712 \n",
            "Epoch 643: training loss 0.752126 \n",
            "Epoch 644: training loss 0.133127 \n",
            "Epoch 645: training loss 3.184422 \n",
            "Epoch 646: training loss 0.155129 \n",
            "Epoch 647: training loss 0.321358 \n",
            "Epoch 648: training loss 0.243059 \n",
            "Epoch 649: training loss 0.405576 \n",
            "Epoch 650: training loss 0.173713 \n",
            "Epoch 651: training loss 0.028840 \n",
            "Epoch 652: training loss 0.383316 \n",
            "Epoch 653: training loss 1.145983 \n",
            "Epoch 654: training loss 0.788997 \n",
            "Epoch 655: training loss 0.371863 \n",
            "Epoch 656: training loss 0.147950 \n",
            "Epoch 657: training loss 0.171200 \n",
            "Epoch 658: training loss 0.196344 \n",
            "Epoch 659: training loss 0.458949 \n",
            "Epoch 660: training loss 0.270606 \n",
            "Epoch 661: training loss 1.006151 \n",
            "Epoch 662: training loss 0.113469 \n",
            "Epoch 663: training loss 2.227960 \n",
            "Epoch 664: training loss 0.144586 \n",
            "Epoch 665: training loss 1.382797 \n",
            "Epoch 666: training loss 0.344586 \n",
            "Epoch 667: training loss 0.258904 \n",
            "Epoch 668: training loss 0.489438 \n",
            "Epoch 669: training loss 0.453843 \n",
            "Epoch 670: training loss 0.796890 \n",
            "Epoch 671: training loss 0.178864 \n",
            "Epoch 672: training loss 0.672807 \n",
            "Epoch 673: training loss 0.253778 \n",
            "Epoch 674: training loss 0.308143 \n",
            "Epoch 675: training loss 2.296913 \n",
            "Epoch 676: training loss 0.227211 \n",
            "Epoch 677: training loss 0.230382 \n",
            "Epoch 678: training loss 0.925083 \n",
            "Epoch 679: training loss 0.504571 \n",
            "Epoch 680: training loss 0.152798 \n",
            "Epoch 681: training loss 0.299003 \n",
            "Epoch 682: training loss 0.717656 \n",
            "Epoch 683: training loss 1.297274 \n",
            "Epoch 684: training loss 0.503191 \n",
            "Epoch 685: training loss 0.346776 \n",
            "Epoch 686: training loss 2.515826 \n",
            "Epoch 687: training loss 0.484859 \n",
            "Epoch 688: training loss 0.094640 \n",
            "Epoch 689: training loss 0.987975 \n",
            "Epoch 690: training loss 0.136406 \n",
            "Epoch 691: training loss 0.271219 \n",
            "Epoch 692: training loss 0.213988 \n",
            "Epoch 693: training loss 0.143865 \n",
            "Epoch 694: training loss 0.371737 \n",
            "Epoch 695: training loss 0.275023 \n",
            "Epoch 696: training loss 0.262360 \n",
            "Epoch 697: training loss 0.252789 \n",
            "Epoch 698: training loss 1.217473 \n",
            "Epoch 699: training loss 0.638297 \n",
            "Epoch 700: training loss 0.789755 \n",
            "Epoch 701: training loss 0.225484 \n",
            "Epoch 702: training loss 1.471531 \n",
            "Epoch 703: training loss 0.155664 \n",
            "Epoch 704: training loss 0.202317 \n",
            "Epoch 705: training loss 0.300644 \n",
            "Epoch 706: training loss 0.153677 \n",
            "Epoch 707: training loss 0.606782 \n",
            "Epoch 708: training loss 0.291970 \n",
            "Epoch 709: training loss 0.157324 \n",
            "Epoch 710: training loss 0.332201 \n",
            "Epoch 711: training loss 0.301721 \n",
            "Epoch 712: training loss 0.227221 \n",
            "Epoch 713: training loss 0.466298 \n",
            "Epoch 714: training loss 0.122024 \n",
            "Epoch 715: training loss 0.740416 \n",
            "Epoch 716: training loss 0.292128 \n",
            "Epoch 717: training loss 0.514254 \n",
            "Epoch 718: training loss 0.105669 \n",
            "Epoch 719: training loss 1.201167 \n",
            "Epoch 720: training loss 1.558304 \n",
            "Epoch 721: training loss 0.123688 \n",
            "Epoch 722: training loss 0.136652 \n",
            "Epoch 723: training loss 0.772519 \n",
            "Epoch 724: training loss 0.128448 \n",
            "Epoch 725: training loss 0.240056 \n",
            "Epoch 726: training loss 0.849900 \n",
            "Epoch 727: training loss 0.241598 \n",
            "Epoch 728: training loss 0.296163 \n",
            "Epoch 729: training loss 0.527495 \n",
            "Epoch 730: training loss 0.233998 \n",
            "Epoch 731: training loss 0.510066 \n",
            "Epoch 732: training loss 0.145380 \n",
            "Epoch 733: training loss 2.200995 \n",
            "Epoch 734: training loss 0.413168 \n",
            "Epoch 735: training loss 0.152197 \n",
            "Epoch 736: training loss 2.732258 \n",
            "Epoch 737: training loss 0.348299 \n",
            "Epoch 738: training loss 0.195684 \n",
            "Epoch 739: training loss 0.078400 \n",
            "Epoch 740: training loss 1.831150 \n",
            "Epoch 741: training loss 0.114787 \n",
            "Epoch 742: training loss 0.213071 \n",
            "Epoch 743: training loss 2.351532 \n",
            "Epoch 744: training loss 0.240229 \n",
            "Epoch 745: training loss 0.418764 \n",
            "Epoch 746: training loss 0.299113 \n",
            "Epoch 747: training loss 0.735024 \n",
            "Epoch 748: training loss 0.326615 \n",
            "Epoch 749: training loss 1.593558 \n",
            "Epoch 750: training loss 0.042695 \n",
            "Epoch 751: training loss 0.139883 \n",
            "Epoch 752: training loss 0.341548 \n",
            "Epoch 753: training loss 0.026820 \n",
            "Epoch 754: training loss 0.654151 \n",
            "Epoch 755: training loss 0.152383 \n",
            "Epoch 756: training loss 1.004921 \n",
            "Epoch 757: training loss 1.808068 \n",
            "Epoch 758: training loss 0.840259 \n",
            "Epoch 759: training loss 0.562415 \n",
            "Epoch 760: training loss 0.926357 \n",
            "Epoch 761: training loss 0.413792 \n",
            "Epoch 762: training loss 0.377119 \n",
            "Epoch 763: training loss 0.500138 \n",
            "Epoch 764: training loss 1.422499 \n",
            "Epoch 765: training loss 0.051472 \n",
            "Epoch 766: training loss 0.175224 \n",
            "Epoch 767: training loss 0.149048 \n",
            "Epoch 768: training loss 0.600150 \n",
            "Epoch 769: training loss 0.315217 \n",
            "Epoch 770: training loss 0.648635 \n",
            "Epoch 771: training loss 0.172386 \n",
            "Epoch 772: training loss 0.335743 \n",
            "Epoch 773: training loss 0.652371 \n",
            "Epoch 774: training loss 0.200220 \n",
            "Epoch 775: training loss 0.125342 \n",
            "Epoch 776: training loss 0.316477 \n",
            "Epoch 777: training loss 0.477417 \n",
            "Epoch 778: training loss 0.175437 \n",
            "Epoch 779: training loss 0.451373 \n",
            "Epoch 780: training loss 0.122827 \n",
            "Epoch 781: training loss 0.023844 \n",
            "Epoch 782: training loss 0.463606 \n",
            "Epoch 783: training loss 0.267505 \n",
            "Epoch 784: training loss 1.367634 \n",
            "Epoch 785: training loss 1.248317 \n",
            "Epoch 786: training loss 0.316452 \n",
            "Epoch 787: training loss 0.134260 \n",
            "Epoch 788: training loss 0.130718 \n",
            "Epoch 789: training loss 0.261057 \n",
            "Epoch 790: training loss 0.951246 \n",
            "Epoch 791: training loss 0.075508 \n",
            "Epoch 792: training loss 0.538968 \n",
            "Epoch 793: training loss 0.229045 \n",
            "Epoch 794: training loss 1.031490 \n",
            "Epoch 795: training loss 0.217041 \n",
            "Epoch 796: training loss 1.962128 \n",
            "Epoch 797: training loss 0.225601 \n",
            "Epoch 798: training loss 0.182514 \n",
            "Epoch 799: training loss 0.697391 \n",
            "Epoch 800: training loss 0.281536 \n",
            "Epoch 801: training loss 0.105750 \n",
            "Epoch 802: training loss 0.555092 \n",
            "Epoch 803: training loss 0.244178 \n",
            "Epoch 804: training loss 0.224403 \n",
            "Epoch 805: training loss 0.063644 \n",
            "Epoch 806: training loss 0.062760 \n",
            "Epoch 807: training loss 0.063834 \n",
            "Epoch 808: training loss 1.514786 \n",
            "Epoch 809: training loss 0.190347 \n",
            "Epoch 810: training loss 0.424129 \n",
            "Epoch 811: training loss 0.161012 \n",
            "Epoch 812: training loss 0.044053 \n",
            "Epoch 813: training loss 0.148940 \n",
            "Epoch 814: training loss 0.090258 \n",
            "Epoch 815: training loss 0.137403 \n",
            "Epoch 816: training loss 0.299328 \n",
            "Epoch 817: training loss 0.474466 \n",
            "Epoch 818: training loss 0.448186 \n",
            "Epoch 819: training loss 0.436519 \n",
            "Epoch 820: training loss 0.351773 \n",
            "Epoch 821: training loss 0.083382 \n",
            "Epoch 822: training loss 0.099908 \n",
            "Epoch 823: training loss 1.142158 \n",
            "Epoch 824: training loss 0.110801 \n",
            "Epoch 825: training loss 0.132678 \n",
            "Epoch 826: training loss 0.481739 \n",
            "Epoch 827: training loss 0.515854 \n",
            "Epoch 828: training loss 0.147900 \n",
            "Epoch 829: training loss 1.330115 \n",
            "Epoch 830: training loss 0.057532 \n",
            "Epoch 831: training loss 1.270866 \n",
            "Epoch 832: training loss 0.204264 \n",
            "Epoch 833: training loss 0.199236 \n",
            "Epoch 834: training loss 0.190431 \n",
            "Epoch 835: training loss 0.099703 \n",
            "Epoch 836: training loss 0.486291 \n",
            "Epoch 837: training loss 0.308690 \n",
            "Epoch 838: training loss 0.217554 \n",
            "Epoch 839: training loss 0.071441 \n",
            "Epoch 840: training loss 0.146604 \n",
            "Epoch 841: training loss 0.816179 \n",
            "Epoch 842: training loss 0.295335 \n",
            "Epoch 843: training loss 0.124453 \n",
            "Epoch 844: training loss 0.107245 \n",
            "Epoch 845: training loss 0.113472 \n",
            "Epoch 846: training loss 1.642430 \n",
            "Epoch 847: training loss 0.938568 \n",
            "Epoch 848: training loss 0.295882 \n",
            "Epoch 849: training loss 0.115094 \n",
            "Epoch 850: training loss 0.138168 \n",
            "Epoch 851: training loss 0.117323 \n",
            "Epoch 852: training loss 0.078052 \n",
            "Epoch 853: training loss 0.942025 \n",
            "Epoch 854: training loss 0.287451 \n",
            "Epoch 855: training loss 0.150761 \n",
            "Epoch 856: training loss 0.037651 \n",
            "Epoch 857: training loss 2.266259 \n",
            "Epoch 858: training loss 0.105937 \n",
            "Epoch 859: training loss 0.500761 \n",
            "Epoch 860: training loss 0.918652 \n",
            "Epoch 861: training loss 0.147819 \n",
            "Epoch 862: training loss 0.056697 \n",
            "Epoch 863: training loss 0.210854 \n",
            "Epoch 864: training loss 0.059413 \n",
            "Epoch 865: training loss 0.779666 \n",
            "Epoch 866: training loss 1.810110 \n",
            "Epoch 867: training loss 0.766737 \n",
            "Epoch 868: training loss 0.103626 \n",
            "Epoch 869: training loss 0.367951 \n",
            "Epoch 870: training loss 0.357351 \n",
            "Epoch 871: training loss 1.176270 \n",
            "Epoch 872: training loss 0.394631 \n",
            "Epoch 873: training loss 0.245409 \n",
            "Epoch 874: training loss 0.228458 \n",
            "Epoch 875: training loss 0.219437 \n",
            "Epoch 876: training loss 0.031637 \n",
            "Epoch 877: training loss 1.068469 \n",
            "Epoch 878: training loss 0.086723 \n",
            "Epoch 879: training loss 0.046956 \n",
            "Epoch 880: training loss 0.845485 \n",
            "Epoch 881: training loss 0.068837 \n",
            "Epoch 882: training loss 0.774357 \n",
            "Epoch 883: training loss 0.272904 \n",
            "Epoch 884: training loss 0.163590 \n",
            "Epoch 885: training loss 0.047009 \n",
            "Epoch 886: training loss 0.588325 \n",
            "Epoch 887: training loss 0.045075 \n",
            "Epoch 888: training loss 0.051789 \n",
            "Epoch 889: training loss 0.281063 \n",
            "Epoch 890: training loss 0.120771 \n",
            "Epoch 891: training loss 0.099850 \n",
            "Epoch 892: training loss 0.438054 \n",
            "Epoch 893: training loss 0.032554 \n",
            "Epoch 894: training loss 0.078060 \n",
            "Epoch 895: training loss 0.727571 \n",
            "Epoch 896: training loss 1.629427 \n",
            "Epoch 897: training loss 0.068080 \n",
            "Epoch 898: training loss 0.230443 \n",
            "Epoch 899: training loss 1.195575 \n",
            "Epoch 900: training loss 0.464582 \n",
            "Epoch 901: training loss 0.404893 \n",
            "Epoch 902: training loss 2.670787 \n",
            "Epoch 903: training loss 0.287991 \n",
            "Epoch 904: training loss 1.590430 \n",
            "Epoch 905: training loss 0.063141 \n",
            "Epoch 906: training loss 0.100869 \n",
            "Epoch 907: training loss 0.114380 \n",
            "Epoch 908: training loss 0.147598 \n",
            "Epoch 909: training loss 0.111480 \n",
            "Epoch 910: training loss 0.103113 \n",
            "Epoch 911: training loss 0.571631 \n",
            "Epoch 912: training loss 0.765649 \n",
            "Epoch 913: training loss 0.446560 \n",
            "Epoch 914: training loss 1.048837 \n",
            "Epoch 915: training loss 0.878137 \n",
            "Epoch 916: training loss 0.145433 \n",
            "Epoch 917: training loss 0.786964 \n",
            "Epoch 918: training loss 0.066849 \n",
            "Epoch 919: training loss 0.097182 \n",
            "Epoch 920: training loss 0.154171 \n",
            "Epoch 921: training loss 0.572795 \n",
            "Epoch 922: training loss 0.066451 \n",
            "Epoch 923: training loss 0.668346 \n",
            "Epoch 924: training loss 0.428695 \n",
            "Epoch 925: training loss 0.145536 \n",
            "Epoch 926: training loss 0.082381 \n",
            "Epoch 927: training loss 1.314108 \n",
            "Epoch 928: training loss 0.260337 \n",
            "Epoch 929: training loss 0.059549 \n",
            "Epoch 930: training loss 0.061007 \n",
            "Epoch 931: training loss 0.010937 \n",
            "Epoch 932: training loss 1.003257 \n",
            "Epoch 933: training loss 1.151193 \n",
            "Epoch 934: training loss 0.725499 \n",
            "Epoch 935: training loss 0.054993 \n",
            "Epoch 936: training loss 1.434882 \n",
            "Epoch 937: training loss 0.034947 \n",
            "Epoch 938: training loss 0.145758 \n",
            "Epoch 939: training loss 0.746902 \n",
            "Epoch 940: training loss 0.409227 \n",
            "Epoch 941: training loss 0.016672 \n",
            "Epoch 942: training loss 0.032378 \n",
            "Epoch 943: training loss 1.059865 \n",
            "Epoch 944: training loss 0.106420 \n",
            "Epoch 945: training loss 0.125869 \n",
            "Epoch 946: training loss 0.146912 \n",
            "Epoch 947: training loss 0.317164 \n",
            "Epoch 948: training loss 0.426875 \n",
            "Epoch 949: training loss 0.294084 \n",
            "Epoch 950: training loss 1.071209 \n",
            "Epoch 951: training loss 0.834238 \n",
            "Epoch 952: training loss 0.238560 \n",
            "Epoch 953: training loss 0.469654 \n",
            "Epoch 954: training loss 0.018113 \n",
            "Epoch 955: training loss 0.749751 \n",
            "Epoch 956: training loss 2.313858 \n",
            "Epoch 957: training loss 0.672110 \n",
            "Epoch 958: training loss 0.013068 \n",
            "Epoch 959: training loss 0.034327 \n",
            "Epoch 960: training loss 1.758858 \n",
            "Epoch 961: training loss 0.281612 \n",
            "Epoch 962: training loss 0.681170 \n",
            "Epoch 963: training loss 0.519310 \n",
            "Epoch 964: training loss 0.269885 \n",
            "Epoch 965: training loss 0.281527 \n",
            "Epoch 966: training loss 0.504353 \n",
            "Epoch 967: training loss 0.183836 \n",
            "Epoch 968: training loss 0.184957 \n",
            "Epoch 969: training loss 0.161533 \n",
            "Epoch 970: training loss 0.140962 \n",
            "Epoch 971: training loss 0.903416 \n",
            "Epoch 972: training loss 0.687864 \n",
            "Epoch 973: training loss 0.047291 \n",
            "Epoch 974: training loss 0.694881 \n",
            "Epoch 975: training loss 0.125120 \n",
            "Epoch 976: training loss 0.234932 \n",
            "Epoch 977: training loss 0.516879 \n",
            "Epoch 978: training loss 0.480169 \n",
            "Epoch 979: training loss 0.353710 \n",
            "Epoch 980: training loss 0.039713 \n",
            "Epoch 981: training loss 0.161549 \n",
            "Epoch 982: training loss 0.181820 \n",
            "Epoch 983: training loss 0.079924 \n",
            "Epoch 984: training loss 0.216745 \n",
            "Epoch 985: training loss 0.174360 \n",
            "Epoch 986: training loss 0.118387 \n",
            "Epoch 987: training loss 0.092038 \n",
            "Epoch 988: training loss 0.215235 \n",
            "Epoch 989: training loss 0.333318 \n",
            "Epoch 990: training loss 2.309917 \n",
            "Epoch 991: training loss 0.641617 \n",
            "Epoch 992: training loss 0.034173 \n",
            "Epoch 993: training loss 0.022058 \n",
            "Epoch 994: training loss 1.875657 \n",
            "Epoch 995: training loss 0.036929 \n",
            "Epoch 996: training loss 0.198001 \n",
            "Epoch 997: training loss 0.376017 \n",
            "Epoch 998: training loss 0.095081 \n",
            "Epoch 999: training loss 0.004902 \n",
            "training acc.. 91.858\n",
            "testing loss.. 90.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 **Bonus: Dropout"
      ],
      "metadata": {
        "id": "vIj1A8hADHKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For each neuron (including inputs), generate a uniform random number between 0 and 1.\n",
        "If the number is greater than Î±, set the neuronâ€™s output to 0.\n",
        "Otherwise, donâ€™t touch the neuronâ€™s output.\n",
        "In backpropogation, gradients of same neurons are also zeroed out\n",
        "\n",
        "change in fprop and backprop functions only\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class NeuralNetwork():\n",
        "\n",
        "    def __init__(self, num_neurons, activations_func, learning_rate, num_epochs):\n",
        "        self.num_neurons = num_neurons    #no. of neurons in each layer\n",
        "        self.activations_func = activations_func    #activation functions of each layer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_epochs\n",
        "        self.num_layers = len(self.num_neurons) - 1\n",
        "        self.parameters = dict()    #dictionary to store weights and biases\n",
        "        self.net = dict()       #dictionary to store (outputs)z values\n",
        "        self.grads = dict()       #dictionary to store gradients\n",
        "\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        print(\"Num of layers\", self.num_layers)\n",
        "        for l in range(1, self.num_layers + 1):\n",
        "            if self.activations_func[l] == 'relu': #xavier intialization method\n",
        "                self.parameters['W%s' % l] = np.random.randn(self.num_neurons[l], self.num_neurons[l-1]) / np.sqrt(self.num_neurons[l-1]/2.)\n",
        "            else:\n",
        "                self.parameters['W%s' % l] = np.random.randn(self.num_neurons[l], self.num_neurons[l-1]) / np.sqrt(self.num_neurons[l - 1])\n",
        "            self.parameters['b%s' % l] = np.zeros((self.num_neurons[l], 1))\n",
        "\n",
        "\n",
        "    def fprop_dropout(self, batch_input):\n",
        "\n",
        "        alpha = 0.85\n",
        "        self.net['z0'] = batch_input\n",
        "        for l in range(1, self.num_layers + 1):\n",
        "            if (l == self.num_layers):\n",
        "                self.net['a%s' % l] = np.add((np.dot(self.parameters['W%s' % l], batch_input)), self.parameters['b%s' % l])\n",
        "                af = self.activations_func[l]\n",
        "                self.net['z%s' % l] = eval(af)(self.net['a%s' % l])\n",
        "                batch_input = self.net['z%s' % l]\n",
        "\n",
        "            else:\n",
        "\n",
        "                self.net['a%s' % l] = np.add((np.dot(self.parameters['W%s' % l], batch_input)), self.parameters['b%s' % l])\n",
        "                af = self.activations_func[l]\n",
        "                self.net['z%s' % l] = eval(af)(self.net['a%s' % l])\n",
        "\n",
        "                self.net['D%s' % l] = np.random.uniform(0.0, 1.0, (self.net['z%s' % l].shape[0], self.net['z%s' % l].shape[1]))\n",
        "\n",
        "                self.net['D%s' % l] = (self.net['D%s' % l] < alpha)\n",
        "                self.net['z%s' % l] = self.net['z%s' % l] * self.net['D%s' % l]\n",
        "\n",
        "                batch_input = self.net['z%s' % l]\n",
        "\n",
        "\n",
        "    def calculate_loss(self, targets):\n",
        "      y = self.net['z%s' % str(self.num_layers)]\n",
        "      loss = (-1/y.shape[1]) * np.sum(targets * np.log(y))\n",
        "      return loss\n",
        "\n",
        "    def update_parameters(self,epoch):\n",
        "       for l in range(1, self.num_layers + 1):\n",
        "           self.parameters['W%s' % l] = self.parameters['W%s' % l] - (self.learning_rate * self.grads['dw%s' % l])\n",
        "           self.parameters['b%s' % l] = self.parameters['b%s' % l] - (self.learning_rate * self.grads['db%s' % l])\n",
        "\n",
        "\n",
        "    def bprop_dropout(self, batch_target):\n",
        "\n",
        "      for l in range(self.num_layers, 0, -1):\n",
        "          if (l==self.num_layers):\n",
        "              self.grads['dz%s' % l] = self.net['z%s' % l] - batch_target\n",
        "              self.grads['dw%s' % l] = np.dot(self.grads['dz%s' % l], self.net['z%s' % str(l-1)].T)\n",
        "              self.grads['db%s' % l] = np.sum(self.grads['dz%s' % l], axis=1, keepdims=True)\n",
        "\n",
        "          else:\n",
        "              af = self.activations_func[l]\n",
        "              if (af == \"tanh\"):\n",
        "                  d = tanh_derivative(self.net['z%s' % l])\n",
        "              elif (af == \"softmax\"):\n",
        "                  d = softmax_derivative(self.net['z%s' % l])\n",
        "              elif (af == \"relu\"):\n",
        "                  d = relu_derivative(self.net['z%s' % l])\n",
        "\n",
        "              self.grads['da%s' % l] = np.dot(self.parameters['W%s' % str(l+1)].T, self.grads['dz%s' % str(l+1)])\n",
        "              self.grads['da%s' % l] = self.grads['da%s' % l] * self.net['D%s' % l]\n",
        "              self.grads['dz%s' % l] = np.multiply((self.grads['da%s' % l]), d)\n",
        "              self.grads['dw%s' % l] = np.dot(self.grads['dz%s' % l], self.net['z%s' % str(l-1)].T)\n",
        "              self.grads['db%s' % l] = np.sum(self.grads['dz%s' % l], axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "    def train(self, train_x, train_y):\n",
        "        train_x, train_y = shuffle(train_x, train_y, random_state=0)\n",
        "        self.initialize_parameters()\n",
        "        train_loss = []\n",
        "\n",
        "\n",
        "        for i in range(0, self.num_iterations):\n",
        "            input = train_x[i].reshape(-1,1)\n",
        "            target = train_y[i].reshape(-1,1)\n",
        "            self.fprop_dropout(input)\n",
        "            loss = self.calculate_loss(target)\n",
        "            self.bprop_dropout(target)\n",
        "            self.update_parameters(i)\n",
        "\n",
        "            train_loss.append(loss)\n",
        "            print(\"Epoch %i: training loss %f \" % (i, loss))\n",
        "\n",
        "    def test(self, test_x, test_t):\n",
        "        self.total_acc = 0\n",
        "        count = 0\n",
        "        predicted = []\n",
        "        true = []\n",
        "        for i in range(test_x.shape[0]):\n",
        "            test_xi = test_x[i]\n",
        "            test_ti = test_t[i]\n",
        "            test_xi = test_xi.reshape(-1,1)\n",
        "            test_ti = test_ti.reshape(-1, 1)\n",
        "            self.fprop_dropout(test_xi)\n",
        "            output = self.net['z%s'% str(self.num_layers)]\n",
        "            if np.argmax(output) == np.argmax(test_ti):\n",
        "                self.total_acc += 1\n",
        "            predicted.append(np.argmax(output))\n",
        "            true.append(np.argmax(test_ti))\n",
        "            count += 1\n",
        "\n",
        "        acc = self.total_acc / count * 100\n",
        "\n",
        "        return acc\n"
      ],
      "metadata": {
        "id": "tPMD8HoLDORK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create l-dim network by just adding num of neurons in layer_dim\n",
        "# first and last elements represent input and output layers dim\n",
        "layer_dim = [64, 128, 64, 10]\n",
        "\n",
        "# add activation functions name here.\n",
        "# input layer activation function is None\n",
        "activations = ['tanh', 'tanh', 'relu', 'softmax']\n",
        "assert len(layer_dim) ==  len(activations), \"layer dim or activation is missing..\"\n",
        "\n",
        "# hyper parameters of neural network\n",
        "learning_rate = 1e-2\n",
        "num_epochs = 1000\n",
        "\n",
        "\n",
        "nn = NeuralNetwork(layer_dim, activations, learning_rate, num_epochs)\n",
        "\n",
        "# train neural network\n",
        "nn.train(X_train, y_train)\n",
        "\n",
        "\n",
        "# test neural network\n",
        "train_acc = nn.test(X_train, y_train)\n",
        "print(\"training acc..\", np.round(train_acc, 4))\n",
        "test_acc = nn.test(X_test, y_test)\n",
        "print(\"testing loss..\", np.round(test_acc, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqL4lCAnJ4Cn",
        "outputId": "ab75b309-6d27-410b-d551-f2fa9d36650a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of layers 3\n",
            "Epoch 0: training loss 2.035187 \n",
            "Epoch 1: training loss 2.468857 \n",
            "Epoch 2: training loss 3.098086 \n",
            "Epoch 3: training loss 2.301437 \n",
            "Epoch 4: training loss 2.276477 \n",
            "Epoch 5: training loss 2.500915 \n",
            "Epoch 6: training loss 2.574380 \n",
            "Epoch 7: training loss 2.135977 \n",
            "Epoch 8: training loss 2.451235 \n",
            "Epoch 9: training loss 2.252740 \n",
            "Epoch 10: training loss 1.992118 \n",
            "Epoch 11: training loss 1.722437 \n",
            "Epoch 12: training loss 2.782472 \n",
            "Epoch 13: training loss 2.022212 \n",
            "Epoch 14: training loss 2.151182 \n",
            "Epoch 15: training loss 2.986844 \n",
            "Epoch 16: training loss 2.113712 \n",
            "Epoch 17: training loss 2.221602 \n",
            "Epoch 18: training loss 2.694100 \n",
            "Epoch 19: training loss 2.036363 \n",
            "Epoch 20: training loss 3.011991 \n",
            "Epoch 21: training loss 2.415263 \n",
            "Epoch 22: training loss 2.307297 \n",
            "Epoch 23: training loss 1.920762 \n",
            "Epoch 24: training loss 1.862155 \n",
            "Epoch 25: training loss 2.741723 \n",
            "Epoch 26: training loss 1.600740 \n",
            "Epoch 27: training loss 2.290796 \n",
            "Epoch 28: training loss 2.271186 \n",
            "Epoch 29: training loss 2.875311 \n",
            "Epoch 30: training loss 2.757929 \n",
            "Epoch 31: training loss 2.517624 \n",
            "Epoch 32: training loss 2.083166 \n",
            "Epoch 33: training loss 1.490439 \n",
            "Epoch 34: training loss 2.466629 \n",
            "Epoch 35: training loss 2.444416 \n",
            "Epoch 36: training loss 2.571563 \n",
            "Epoch 37: training loss 2.363235 \n",
            "Epoch 38: training loss 2.188955 \n",
            "Epoch 39: training loss 2.098340 \n",
            "Epoch 40: training loss 2.034936 \n",
            "Epoch 41: training loss 2.149827 \n",
            "Epoch 42: training loss 2.157103 \n",
            "Epoch 43: training loss 1.531004 \n",
            "Epoch 44: training loss 1.917774 \n",
            "Epoch 45: training loss 2.423011 \n",
            "Epoch 46: training loss 2.165483 \n",
            "Epoch 47: training loss 2.569554 \n",
            "Epoch 48: training loss 3.019396 \n",
            "Epoch 49: training loss 1.503195 \n",
            "Epoch 50: training loss 2.267735 \n",
            "Epoch 51: training loss 2.296312 \n",
            "Epoch 52: training loss 1.488925 \n",
            "Epoch 53: training loss 1.857500 \n",
            "Epoch 54: training loss 2.201347 \n",
            "Epoch 55: training loss 2.570295 \n",
            "Epoch 56: training loss 1.726184 \n",
            "Epoch 57: training loss 2.172137 \n",
            "Epoch 58: training loss 2.783917 \n",
            "Epoch 59: training loss 2.191710 \n",
            "Epoch 60: training loss 2.156836 \n",
            "Epoch 61: training loss 1.999468 \n",
            "Epoch 62: training loss 1.898983 \n",
            "Epoch 63: training loss 2.018193 \n",
            "Epoch 64: training loss 2.341991 \n",
            "Epoch 65: training loss 2.209837 \n",
            "Epoch 66: training loss 1.946264 \n",
            "Epoch 67: training loss 1.640511 \n",
            "Epoch 68: training loss 1.443841 \n",
            "Epoch 69: training loss 2.322185 \n",
            "Epoch 70: training loss 2.226433 \n",
            "Epoch 71: training loss 3.264322 \n",
            "Epoch 72: training loss 2.158396 \n",
            "Epoch 73: training loss 2.434362 \n",
            "Epoch 74: training loss 2.636485 \n",
            "Epoch 75: training loss 2.167169 \n",
            "Epoch 76: training loss 2.889227 \n",
            "Epoch 77: training loss 1.581103 \n",
            "Epoch 78: training loss 2.341863 \n",
            "Epoch 79: training loss 2.206993 \n",
            "Epoch 80: training loss 2.952866 \n",
            "Epoch 81: training loss 2.402166 \n",
            "Epoch 82: training loss 2.620303 \n",
            "Epoch 83: training loss 2.649717 \n",
            "Epoch 84: training loss 1.948649 \n",
            "Epoch 85: training loss 2.580596 \n",
            "Epoch 86: training loss 1.919206 \n",
            "Epoch 87: training loss 2.250125 \n",
            "Epoch 88: training loss 2.477685 \n",
            "Epoch 89: training loss 2.209951 \n",
            "Epoch 90: training loss 2.256686 \n",
            "Epoch 91: training loss 2.015104 \n",
            "Epoch 92: training loss 2.223145 \n",
            "Epoch 93: training loss 2.055803 \n",
            "Epoch 94: training loss 2.427474 \n",
            "Epoch 95: training loss 2.133767 \n",
            "Epoch 96: training loss 1.548274 \n",
            "Epoch 97: training loss 1.793132 \n",
            "Epoch 98: training loss 2.483404 \n",
            "Epoch 99: training loss 2.161316 \n",
            "Epoch 100: training loss 1.705311 \n",
            "Epoch 101: training loss 2.574226 \n",
            "Epoch 102: training loss 2.517383 \n",
            "Epoch 103: training loss 1.605719 \n",
            "Epoch 104: training loss 1.990721 \n",
            "Epoch 105: training loss 2.126421 \n",
            "Epoch 106: training loss 2.089152 \n",
            "Epoch 107: training loss 1.963314 \n",
            "Epoch 108: training loss 2.583202 \n",
            "Epoch 109: training loss 2.070677 \n",
            "Epoch 110: training loss 2.042601 \n",
            "Epoch 111: training loss 1.975588 \n",
            "Epoch 112: training loss 1.908546 \n",
            "Epoch 113: training loss 1.833204 \n",
            "Epoch 114: training loss 2.269772 \n",
            "Epoch 115: training loss 2.081259 \n",
            "Epoch 116: training loss 1.854602 \n",
            "Epoch 117: training loss 2.293837 \n",
            "Epoch 118: training loss 2.450449 \n",
            "Epoch 119: training loss 1.782653 \n",
            "Epoch 120: training loss 2.672837 \n",
            "Epoch 121: training loss 1.794520 \n",
            "Epoch 122: training loss 1.411395 \n",
            "Epoch 123: training loss 2.190614 \n",
            "Epoch 124: training loss 2.195245 \n",
            "Epoch 125: training loss 1.697657 \n",
            "Epoch 126: training loss 1.899111 \n",
            "Epoch 127: training loss 2.839379 \n",
            "Epoch 128: training loss 2.398150 \n",
            "Epoch 129: training loss 2.297389 \n",
            "Epoch 130: training loss 1.711467 \n",
            "Epoch 131: training loss 1.841108 \n",
            "Epoch 132: training loss 2.204786 \n",
            "Epoch 133: training loss 2.702300 \n",
            "Epoch 134: training loss 1.750126 \n",
            "Epoch 135: training loss 2.461957 \n",
            "Epoch 136: training loss 2.224730 \n",
            "Epoch 137: training loss 2.597136 \n",
            "Epoch 138: training loss 1.613692 \n",
            "Epoch 139: training loss 2.325532 \n",
            "Epoch 140: training loss 1.814595 \n",
            "Epoch 141: training loss 2.547982 \n",
            "Epoch 142: training loss 2.245645 \n",
            "Epoch 143: training loss 2.797273 \n",
            "Epoch 144: training loss 1.246885 \n",
            "Epoch 145: training loss 2.260955 \n",
            "Epoch 146: training loss 2.174039 \n",
            "Epoch 147: training loss 2.265729 \n",
            "Epoch 148: training loss 2.421024 \n",
            "Epoch 149: training loss 2.274871 \n",
            "Epoch 150: training loss 1.624525 \n",
            "Epoch 151: training loss 1.756621 \n",
            "Epoch 152: training loss 2.208341 \n",
            "Epoch 153: training loss 2.087139 \n",
            "Epoch 154: training loss 2.198392 \n",
            "Epoch 155: training loss 1.272764 \n",
            "Epoch 156: training loss 2.649490 \n",
            "Epoch 157: training loss 2.319247 \n",
            "Epoch 158: training loss 2.388827 \n",
            "Epoch 159: training loss 1.702945 \n",
            "Epoch 160: training loss 2.245250 \n",
            "Epoch 161: training loss 1.785209 \n",
            "Epoch 162: training loss 2.334422 \n",
            "Epoch 163: training loss 2.291152 \n",
            "Epoch 164: training loss 2.321782 \n",
            "Epoch 165: training loss 2.235975 \n",
            "Epoch 166: training loss 1.991856 \n",
            "Epoch 167: training loss 2.482977 \n",
            "Epoch 168: training loss 1.834884 \n",
            "Epoch 169: training loss 1.591302 \n",
            "Epoch 170: training loss 2.215101 \n",
            "Epoch 171: training loss 1.660030 \n",
            "Epoch 172: training loss 2.353844 \n",
            "Epoch 173: training loss 2.142128 \n",
            "Epoch 174: training loss 2.300809 \n",
            "Epoch 175: training loss 2.164433 \n",
            "Epoch 176: training loss 2.206284 \n",
            "Epoch 177: training loss 2.048289 \n",
            "Epoch 178: training loss 2.229680 \n",
            "Epoch 179: training loss 2.508094 \n",
            "Epoch 180: training loss 1.980467 \n",
            "Epoch 181: training loss 2.506336 \n",
            "Epoch 182: training loss 2.410979 \n",
            "Epoch 183: training loss 2.098713 \n",
            "Epoch 184: training loss 2.069305 \n",
            "Epoch 185: training loss 1.845477 \n",
            "Epoch 186: training loss 2.188624 \n",
            "Epoch 187: training loss 2.008049 \n",
            "Epoch 188: training loss 1.942683 \n",
            "Epoch 189: training loss 1.513194 \n",
            "Epoch 190: training loss 2.388180 \n",
            "Epoch 191: training loss 2.525172 \n",
            "Epoch 192: training loss 2.493296 \n",
            "Epoch 193: training loss 2.253630 \n",
            "Epoch 194: training loss 1.776863 \n",
            "Epoch 195: training loss 1.210156 \n",
            "Epoch 196: training loss 2.445965 \n",
            "Epoch 197: training loss 1.295855 \n",
            "Epoch 198: training loss 2.741031 \n",
            "Epoch 199: training loss 1.886494 \n",
            "Epoch 200: training loss 2.251372 \n",
            "Epoch 201: training loss 2.229471 \n",
            "Epoch 202: training loss 2.102708 \n",
            "Epoch 203: training loss 2.010442 \n",
            "Epoch 204: training loss 2.366461 \n",
            "Epoch 205: training loss 2.305734 \n",
            "Epoch 206: training loss 1.800743 \n",
            "Epoch 207: training loss 2.196560 \n",
            "Epoch 208: training loss 2.009465 \n",
            "Epoch 209: training loss 2.357699 \n",
            "Epoch 210: training loss 1.148763 \n",
            "Epoch 211: training loss 1.900994 \n",
            "Epoch 212: training loss 2.179285 \n",
            "Epoch 213: training loss 2.144577 \n",
            "Epoch 214: training loss 1.940577 \n",
            "Epoch 215: training loss 0.934041 \n",
            "Epoch 216: training loss 1.107890 \n",
            "Epoch 217: training loss 2.447840 \n",
            "Epoch 218: training loss 2.187479 \n",
            "Epoch 219: training loss 1.319609 \n",
            "Epoch 220: training loss 1.527656 \n",
            "Epoch 221: training loss 2.159728 \n",
            "Epoch 222: training loss 2.198038 \n",
            "Epoch 223: training loss 2.273984 \n",
            "Epoch 224: training loss 1.280812 \n",
            "Epoch 225: training loss 0.630587 \n",
            "Epoch 226: training loss 2.435842 \n",
            "Epoch 227: training loss 1.208267 \n",
            "Epoch 228: training loss 1.316993 \n",
            "Epoch 229: training loss 1.702837 \n",
            "Epoch 230: training loss 2.123910 \n",
            "Epoch 231: training loss 2.217455 \n",
            "Epoch 232: training loss 1.748275 \n",
            "Epoch 233: training loss 2.001777 \n",
            "Epoch 234: training loss 1.473784 \n",
            "Epoch 235: training loss 0.698358 \n",
            "Epoch 236: training loss 2.221172 \n",
            "Epoch 237: training loss 1.962654 \n",
            "Epoch 238: training loss 1.680843 \n",
            "Epoch 239: training loss 2.004832 \n",
            "Epoch 240: training loss 1.858549 \n",
            "Epoch 241: training loss 1.873524 \n",
            "Epoch 242: training loss 1.685876 \n",
            "Epoch 243: training loss 1.039618 \n",
            "Epoch 244: training loss 1.987586 \n",
            "Epoch 245: training loss 1.596843 \n",
            "Epoch 246: training loss 1.236690 \n",
            "Epoch 247: training loss 2.073517 \n",
            "Epoch 248: training loss 2.250728 \n",
            "Epoch 249: training loss 1.711057 \n",
            "Epoch 250: training loss 1.935235 \n",
            "Epoch 251: training loss 2.164693 \n",
            "Epoch 252: training loss 1.230902 \n",
            "Epoch 253: training loss 1.751591 \n",
            "Epoch 254: training loss 2.271607 \n",
            "Epoch 255: training loss 2.848923 \n",
            "Epoch 256: training loss 1.527992 \n",
            "Epoch 257: training loss 1.352422 \n",
            "Epoch 258: training loss 1.954071 \n",
            "Epoch 259: training loss 1.886992 \n",
            "Epoch 260: training loss 2.217109 \n",
            "Epoch 261: training loss 1.238737 \n",
            "Epoch 262: training loss 1.955899 \n",
            "Epoch 263: training loss 0.739315 \n",
            "Epoch 264: training loss 1.992488 \n",
            "Epoch 265: training loss 1.882551 \n",
            "Epoch 266: training loss 1.714979 \n",
            "Epoch 267: training loss 2.265738 \n",
            "Epoch 268: training loss 1.692906 \n",
            "Epoch 269: training loss 1.672018 \n",
            "Epoch 270: training loss 2.205801 \n",
            "Epoch 271: training loss 1.935936 \n",
            "Epoch 272: training loss 1.996799 \n",
            "Epoch 273: training loss 2.080658 \n",
            "Epoch 274: training loss 1.769251 \n",
            "Epoch 275: training loss 2.124101 \n",
            "Epoch 276: training loss 1.148233 \n",
            "Epoch 277: training loss 1.735832 \n",
            "Epoch 278: training loss 1.648165 \n",
            "Epoch 279: training loss 2.276715 \n",
            "Epoch 280: training loss 2.329522 \n",
            "Epoch 281: training loss 2.036121 \n",
            "Epoch 282: training loss 1.869060 \n",
            "Epoch 283: training loss 1.601215 \n",
            "Epoch 284: training loss 2.409363 \n",
            "Epoch 285: training loss 2.842817 \n",
            "Epoch 286: training loss 1.723594 \n",
            "Epoch 287: training loss 2.373259 \n",
            "Epoch 288: training loss 2.326021 \n",
            "Epoch 289: training loss 1.517822 \n",
            "Epoch 290: training loss 1.713261 \n",
            "Epoch 291: training loss 1.256373 \n",
            "Epoch 292: training loss 1.259296 \n",
            "Epoch 293: training loss 2.211062 \n",
            "Epoch 294: training loss 1.876056 \n",
            "Epoch 295: training loss 1.851070 \n",
            "Epoch 296: training loss 1.797792 \n",
            "Epoch 297: training loss 1.503428 \n",
            "Epoch 298: training loss 1.960579 \n",
            "Epoch 299: training loss 1.319636 \n",
            "Epoch 300: training loss 2.010281 \n",
            "Epoch 301: training loss 2.204023 \n",
            "Epoch 302: training loss 1.858361 \n",
            "Epoch 303: training loss 1.445522 \n",
            "Epoch 304: training loss 1.970425 \n",
            "Epoch 305: training loss 1.757533 \n",
            "Epoch 306: training loss 1.392607 \n",
            "Epoch 307: training loss 1.165777 \n",
            "Epoch 308: training loss 1.892505 \n",
            "Epoch 309: training loss 1.321050 \n",
            "Epoch 310: training loss 2.385535 \n",
            "Epoch 311: training loss 1.951551 \n",
            "Epoch 312: training loss 0.914912 \n",
            "Epoch 313: training loss 1.075502 \n",
            "Epoch 314: training loss 1.034841 \n",
            "Epoch 315: training loss 1.963252 \n",
            "Epoch 316: training loss 0.971502 \n",
            "Epoch 317: training loss 2.318945 \n",
            "Epoch 318: training loss 2.190380 \n",
            "Epoch 319: training loss 1.784406 \n",
            "Epoch 320: training loss 2.024322 \n",
            "Epoch 321: training loss 1.903053 \n",
            "Epoch 322: training loss 2.221976 \n",
            "Epoch 323: training loss 1.611997 \n",
            "Epoch 324: training loss 0.927233 \n",
            "Epoch 325: training loss 1.533960 \n",
            "Epoch 326: training loss 1.050957 \n",
            "Epoch 327: training loss 1.086193 \n",
            "Epoch 328: training loss 2.423583 \n",
            "Epoch 329: training loss 1.101828 \n",
            "Epoch 330: training loss 1.587137 \n",
            "Epoch 331: training loss 1.987528 \n",
            "Epoch 332: training loss 1.660812 \n",
            "Epoch 333: training loss 2.031892 \n",
            "Epoch 334: training loss 0.963511 \n",
            "Epoch 335: training loss 0.932912 \n",
            "Epoch 336: training loss 0.867917 \n",
            "Epoch 337: training loss 1.576793 \n",
            "Epoch 338: training loss 2.290241 \n",
            "Epoch 339: training loss 1.751484 \n",
            "Epoch 340: training loss 2.540558 \n",
            "Epoch 341: training loss 1.989989 \n",
            "Epoch 342: training loss 1.217585 \n",
            "Epoch 343: training loss 2.853754 \n",
            "Epoch 344: training loss 1.949297 \n",
            "Epoch 345: training loss 1.554969 \n",
            "Epoch 346: training loss 1.783844 \n",
            "Epoch 347: training loss 2.263452 \n",
            "Epoch 348: training loss 2.419143 \n",
            "Epoch 349: training loss 0.971994 \n",
            "Epoch 350: training loss 1.642561 \n",
            "Epoch 351: training loss 2.189545 \n",
            "Epoch 352: training loss 1.300721 \n",
            "Epoch 353: training loss 1.696032 \n",
            "Epoch 354: training loss 2.684821 \n",
            "Epoch 355: training loss 1.217803 \n",
            "Epoch 356: training loss 0.800165 \n",
            "Epoch 357: training loss 2.531411 \n",
            "Epoch 358: training loss 1.623142 \n",
            "Epoch 359: training loss 2.683338 \n",
            "Epoch 360: training loss 0.965259 \n",
            "Epoch 361: training loss 1.384794 \n",
            "Epoch 362: training loss 2.161503 \n",
            "Epoch 363: training loss 1.281329 \n",
            "Epoch 364: training loss 2.260056 \n",
            "Epoch 365: training loss 1.241339 \n",
            "Epoch 366: training loss 1.315600 \n",
            "Epoch 367: training loss 1.900794 \n",
            "Epoch 368: training loss 1.786200 \n",
            "Epoch 369: training loss 2.567331 \n",
            "Epoch 370: training loss 0.600343 \n",
            "Epoch 371: training loss 2.796401 \n",
            "Epoch 372: training loss 2.755407 \n",
            "Epoch 373: training loss 2.123094 \n",
            "Epoch 374: training loss 1.974314 \n",
            "Epoch 375: training loss 1.451961 \n",
            "Epoch 376: training loss 1.905052 \n",
            "Epoch 377: training loss 2.071876 \n",
            "Epoch 378: training loss 1.464955 \n",
            "Epoch 379: training loss 1.480913 \n",
            "Epoch 380: training loss 2.045623 \n",
            "Epoch 381: training loss 1.689128 \n",
            "Epoch 382: training loss 1.916558 \n",
            "Epoch 383: training loss 2.035239 \n",
            "Epoch 384: training loss 1.596353 \n",
            "Epoch 385: training loss 1.046840 \n",
            "Epoch 386: training loss 1.699499 \n",
            "Epoch 387: training loss 1.762567 \n",
            "Epoch 388: training loss 1.572068 \n",
            "Epoch 389: training loss 1.734203 \n",
            "Epoch 390: training loss 1.990837 \n",
            "Epoch 391: training loss 2.504584 \n",
            "Epoch 392: training loss 1.733033 \n",
            "Epoch 393: training loss 1.423583 \n",
            "Epoch 394: training loss 1.749845 \n",
            "Epoch 395: training loss 1.621896 \n",
            "Epoch 396: training loss 1.814558 \n",
            "Epoch 397: training loss 1.418318 \n",
            "Epoch 398: training loss 1.144604 \n",
            "Epoch 399: training loss 1.657739 \n",
            "Epoch 400: training loss 0.600105 \n",
            "Epoch 401: training loss 1.490153 \n",
            "Epoch 402: training loss 0.644892 \n",
            "Epoch 403: training loss 1.654422 \n",
            "Epoch 404: training loss 1.762009 \n",
            "Epoch 405: training loss 1.766984 \n",
            "Epoch 406: training loss 1.709716 \n",
            "Epoch 407: training loss 1.504565 \n",
            "Epoch 408: training loss 0.927576 \n",
            "Epoch 409: training loss 1.872902 \n",
            "Epoch 410: training loss 2.287029 \n",
            "Epoch 411: training loss 0.800551 \n",
            "Epoch 412: training loss 1.928309 \n",
            "Epoch 413: training loss 0.613210 \n",
            "Epoch 414: training loss 1.787372 \n",
            "Epoch 415: training loss 1.392522 \n",
            "Epoch 416: training loss 0.889072 \n",
            "Epoch 417: training loss 1.082005 \n",
            "Epoch 418: training loss 1.506570 \n",
            "Epoch 419: training loss 2.070366 \n",
            "Epoch 420: training loss 1.421653 \n",
            "Epoch 421: training loss 2.930429 \n",
            "Epoch 422: training loss 1.368590 \n",
            "Epoch 423: training loss 1.297743 \n",
            "Epoch 424: training loss 1.562770 \n",
            "Epoch 425: training loss 2.558455 \n",
            "Epoch 426: training loss 0.940707 \n",
            "Epoch 427: training loss 0.664250 \n",
            "Epoch 428: training loss 0.764181 \n",
            "Epoch 429: training loss 1.591138 \n",
            "Epoch 430: training loss 0.914587 \n",
            "Epoch 431: training loss 2.221555 \n",
            "Epoch 432: training loss 2.323052 \n",
            "Epoch 433: training loss 1.619582 \n",
            "Epoch 434: training loss 2.138349 \n",
            "Epoch 435: training loss 0.987207 \n",
            "Epoch 436: training loss 1.356791 \n",
            "Epoch 437: training loss 2.203199 \n",
            "Epoch 438: training loss 1.888280 \n",
            "Epoch 439: training loss 0.536955 \n",
            "Epoch 440: training loss 2.487463 \n",
            "Epoch 441: training loss 1.998336 \n",
            "Epoch 442: training loss 0.959380 \n",
            "Epoch 443: training loss 1.129242 \n",
            "Epoch 444: training loss 1.538002 \n",
            "Epoch 445: training loss 2.893384 \n",
            "Epoch 446: training loss 2.056228 \n",
            "Epoch 447: training loss 1.662192 \n",
            "Epoch 448: training loss 1.238082 \n",
            "Epoch 449: training loss 1.416146 \n",
            "Epoch 450: training loss 0.912119 \n",
            "Epoch 451: training loss 1.470444 \n",
            "Epoch 452: training loss 0.748665 \n",
            "Epoch 453: training loss 2.451640 \n",
            "Epoch 454: training loss 1.085710 \n",
            "Epoch 455: training loss 1.302200 \n",
            "Epoch 456: training loss 1.296879 \n",
            "Epoch 457: training loss 1.159671 \n",
            "Epoch 458: training loss 1.542456 \n",
            "Epoch 459: training loss 1.132574 \n",
            "Epoch 460: training loss 1.577583 \n",
            "Epoch 461: training loss 0.927558 \n",
            "Epoch 462: training loss 0.902061 \n",
            "Epoch 463: training loss 2.355483 \n",
            "Epoch 464: training loss 0.914031 \n",
            "Epoch 465: training loss 1.149996 \n",
            "Epoch 466: training loss 1.652633 \n",
            "Epoch 467: training loss 1.625575 \n",
            "Epoch 468: training loss 1.569205 \n",
            "Epoch 469: training loss 1.103631 \n",
            "Epoch 470: training loss 1.710105 \n",
            "Epoch 471: training loss 1.351105 \n",
            "Epoch 472: training loss 1.109334 \n",
            "Epoch 473: training loss 2.265262 \n",
            "Epoch 474: training loss 0.948718 \n",
            "Epoch 475: training loss 1.159297 \n",
            "Epoch 476: training loss 0.879762 \n",
            "Epoch 477: training loss 2.907481 \n",
            "Epoch 478: training loss 1.291181 \n",
            "Epoch 479: training loss 1.244694 \n",
            "Epoch 480: training loss 2.343746 \n",
            "Epoch 481: training loss 0.771250 \n",
            "Epoch 482: training loss 0.940294 \n",
            "Epoch 483: training loss 2.174223 \n",
            "Epoch 484: training loss 0.429809 \n",
            "Epoch 485: training loss 1.844747 \n",
            "Epoch 486: training loss 1.175342 \n",
            "Epoch 487: training loss 1.552241 \n",
            "Epoch 488: training loss 0.831498 \n",
            "Epoch 489: training loss 0.851417 \n",
            "Epoch 490: training loss 1.251134 \n",
            "Epoch 491: training loss 0.498761 \n",
            "Epoch 492: training loss 2.520411 \n",
            "Epoch 493: training loss 1.506553 \n",
            "Epoch 494: training loss 2.563800 \n",
            "Epoch 495: training loss 2.279246 \n",
            "Epoch 496: training loss 1.944040 \n",
            "Epoch 497: training loss 1.327059 \n",
            "Epoch 498: training loss 1.500979 \n",
            "Epoch 499: training loss 1.278261 \n",
            "Epoch 500: training loss 0.630598 \n",
            "Epoch 501: training loss 0.671228 \n",
            "Epoch 502: training loss 2.433619 \n",
            "Epoch 503: training loss 0.589331 \n",
            "Epoch 504: training loss 0.963360 \n",
            "Epoch 505: training loss 1.942988 \n",
            "Epoch 506: training loss 1.163048 \n",
            "Epoch 507: training loss 2.372343 \n",
            "Epoch 508: training loss 1.152580 \n",
            "Epoch 509: training loss 1.887430 \n",
            "Epoch 510: training loss 2.566525 \n",
            "Epoch 511: training loss 0.898042 \n",
            "Epoch 512: training loss 0.719968 \n",
            "Epoch 513: training loss 1.058863 \n",
            "Epoch 514: training loss 1.686602 \n",
            "Epoch 515: training loss 2.039555 \n",
            "Epoch 516: training loss 1.712743 \n",
            "Epoch 517: training loss 1.107776 \n",
            "Epoch 518: training loss 1.282966 \n",
            "Epoch 519: training loss 0.811258 \n",
            "Epoch 520: training loss 0.952043 \n",
            "Epoch 521: training loss 1.996286 \n",
            "Epoch 522: training loss 1.763051 \n",
            "Epoch 523: training loss 0.734114 \n",
            "Epoch 524: training loss 0.665974 \n",
            "Epoch 525: training loss 1.439016 \n",
            "Epoch 526: training loss 0.354390 \n",
            "Epoch 527: training loss 0.534237 \n",
            "Epoch 528: training loss 1.376282 \n",
            "Epoch 529: training loss 2.352883 \n",
            "Epoch 530: training loss 0.916312 \n",
            "Epoch 531: training loss 1.749839 \n",
            "Epoch 532: training loss 1.595274 \n",
            "Epoch 533: training loss 0.801424 \n",
            "Epoch 534: training loss 1.856035 \n",
            "Epoch 535: training loss 1.534218 \n",
            "Epoch 536: training loss 1.594761 \n",
            "Epoch 537: training loss 1.095016 \n",
            "Epoch 538: training loss 1.679658 \n",
            "Epoch 539: training loss 1.402288 \n",
            "Epoch 540: training loss 2.482396 \n",
            "Epoch 541: training loss 1.738353 \n",
            "Epoch 542: training loss 1.619943 \n",
            "Epoch 543: training loss 1.216379 \n",
            "Epoch 544: training loss 0.267968 \n",
            "Epoch 545: training loss 1.460578 \n",
            "Epoch 546: training loss 1.035135 \n",
            "Epoch 547: training loss 2.135305 \n",
            "Epoch 548: training loss 0.877220 \n",
            "Epoch 549: training loss 0.684248 \n",
            "Epoch 550: training loss 0.996106 \n",
            "Epoch 551: training loss 0.942931 \n",
            "Epoch 552: training loss 2.589440 \n",
            "Epoch 553: training loss 1.959362 \n",
            "Epoch 554: training loss 1.348601 \n",
            "Epoch 555: training loss 1.694567 \n",
            "Epoch 556: training loss 0.880677 \n",
            "Epoch 557: training loss 1.820194 \n",
            "Epoch 558: training loss 1.419798 \n",
            "Epoch 559: training loss 0.422258 \n",
            "Epoch 560: training loss 0.945192 \n",
            "Epoch 561: training loss 1.643123 \n",
            "Epoch 562: training loss 0.721469 \n",
            "Epoch 563: training loss 0.578511 \n",
            "Epoch 564: training loss 1.701008 \n",
            "Epoch 565: training loss 1.998661 \n",
            "Epoch 566: training loss 0.897524 \n",
            "Epoch 567: training loss 1.240761 \n",
            "Epoch 568: training loss 1.965549 \n",
            "Epoch 569: training loss 0.959707 \n",
            "Epoch 570: training loss 1.088816 \n",
            "Epoch 571: training loss 2.162444 \n",
            "Epoch 572: training loss 1.024211 \n",
            "Epoch 573: training loss 0.760314 \n",
            "Epoch 574: training loss 0.452091 \n",
            "Epoch 575: training loss 1.146317 \n",
            "Epoch 576: training loss 1.171871 \n",
            "Epoch 577: training loss 0.922224 \n",
            "Epoch 578: training loss 0.359150 \n",
            "Epoch 579: training loss 1.195719 \n",
            "Epoch 580: training loss 2.173041 \n",
            "Epoch 581: training loss 0.874942 \n",
            "Epoch 582: training loss 1.062567 \n",
            "Epoch 583: training loss 2.047675 \n",
            "Epoch 584: training loss 2.031816 \n",
            "Epoch 585: training loss 0.954779 \n",
            "Epoch 586: training loss 2.669231 \n",
            "Epoch 587: training loss 0.877264 \n",
            "Epoch 588: training loss 1.199741 \n",
            "Epoch 589: training loss 0.421500 \n",
            "Epoch 590: training loss 0.555616 \n",
            "Epoch 591: training loss 0.504710 \n",
            "Epoch 592: training loss 1.444414 \n",
            "Epoch 593: training loss 0.645129 \n",
            "Epoch 594: training loss 0.425285 \n",
            "Epoch 595: training loss 1.098821 \n",
            "Epoch 596: training loss 1.875858 \n",
            "Epoch 597: training loss 0.385762 \n",
            "Epoch 598: training loss 1.790509 \n",
            "Epoch 599: training loss 1.324913 \n",
            "Epoch 600: training loss 0.834137 \n",
            "Epoch 601: training loss 1.063511 \n",
            "Epoch 602: training loss 2.222432 \n",
            "Epoch 603: training loss 1.074081 \n",
            "Epoch 604: training loss 1.807527 \n",
            "Epoch 605: training loss 0.868733 \n",
            "Epoch 606: training loss 2.202145 \n",
            "Epoch 607: training loss 0.809273 \n",
            "Epoch 608: training loss 2.091352 \n",
            "Epoch 609: training loss 1.067385 \n",
            "Epoch 610: training loss 1.315880 \n",
            "Epoch 611: training loss 0.222263 \n",
            "Epoch 612: training loss 0.407842 \n",
            "Epoch 613: training loss 0.333893 \n",
            "Epoch 614: training loss 1.125927 \n",
            "Epoch 615: training loss 0.325874 \n",
            "Epoch 616: training loss 1.442386 \n",
            "Epoch 617: training loss 0.361275 \n",
            "Epoch 618: training loss 0.156431 \n",
            "Epoch 619: training loss 0.206558 \n",
            "Epoch 620: training loss 2.199790 \n",
            "Epoch 621: training loss 0.942692 \n",
            "Epoch 622: training loss 1.262534 \n",
            "Epoch 623: training loss 1.503119 \n",
            "Epoch 624: training loss 0.566585 \n",
            "Epoch 625: training loss 1.975842 \n",
            "Epoch 626: training loss 1.108152 \n",
            "Epoch 627: training loss 1.809420 \n",
            "Epoch 628: training loss 0.576427 \n",
            "Epoch 629: training loss 0.144671 \n",
            "Epoch 630: training loss 0.855710 \n",
            "Epoch 631: training loss 0.505777 \n",
            "Epoch 632: training loss 1.572165 \n",
            "Epoch 633: training loss 0.225252 \n",
            "Epoch 634: training loss 0.350590 \n",
            "Epoch 635: training loss 1.295853 \n",
            "Epoch 636: training loss 2.078977 \n",
            "Epoch 637: training loss 1.533063 \n",
            "Epoch 638: training loss 2.133695 \n",
            "Epoch 639: training loss 1.019262 \n",
            "Epoch 640: training loss 0.473371 \n",
            "Epoch 641: training loss 1.102473 \n",
            "Epoch 642: training loss 2.286693 \n",
            "Epoch 643: training loss 1.400068 \n",
            "Epoch 644: training loss 0.507264 \n",
            "Epoch 645: training loss 2.012257 \n",
            "Epoch 646: training loss 0.456641 \n",
            "Epoch 647: training loss 1.381787 \n",
            "Epoch 648: training loss 0.517855 \n",
            "Epoch 649: training loss 1.401902 \n",
            "Epoch 650: training loss 0.693373 \n",
            "Epoch 651: training loss 0.158738 \n",
            "Epoch 652: training loss 1.209208 \n",
            "Epoch 653: training loss 2.009263 \n",
            "Epoch 654: training loss 1.563722 \n",
            "Epoch 655: training loss 0.462771 \n",
            "Epoch 656: training loss 0.540605 \n",
            "Epoch 657: training loss 0.666333 \n",
            "Epoch 658: training loss 0.824099 \n",
            "Epoch 659: training loss 0.296389 \n",
            "Epoch 660: training loss 0.266766 \n",
            "Epoch 661: training loss 0.932530 \n",
            "Epoch 662: training loss 0.571227 \n",
            "Epoch 663: training loss 2.106492 \n",
            "Epoch 664: training loss 0.434220 \n",
            "Epoch 665: training loss 1.697273 \n",
            "Epoch 666: training loss 1.025927 \n",
            "Epoch 667: training loss 1.041539 \n",
            "Epoch 668: training loss 0.949434 \n",
            "Epoch 669: training loss 1.578703 \n",
            "Epoch 670: training loss 1.948225 \n",
            "Epoch 671: training loss 0.420970 \n",
            "Epoch 672: training loss 1.085349 \n",
            "Epoch 673: training loss 0.971648 \n",
            "Epoch 674: training loss 2.048364 \n",
            "Epoch 675: training loss 1.979636 \n",
            "Epoch 676: training loss 0.294164 \n",
            "Epoch 677: training loss 1.269628 \n",
            "Epoch 678: training loss 2.115838 \n",
            "Epoch 679: training loss 0.880863 \n",
            "Epoch 680: training loss 0.822625 \n",
            "Epoch 681: training loss 1.263123 \n",
            "Epoch 682: training loss 1.121547 \n",
            "Epoch 683: training loss 0.966209 \n",
            "Epoch 684: training loss 1.044787 \n",
            "Epoch 685: training loss 0.555184 \n",
            "Epoch 686: training loss 1.321119 \n",
            "Epoch 687: training loss 0.859101 \n",
            "Epoch 688: training loss 0.541045 \n",
            "Epoch 689: training loss 1.754965 \n",
            "Epoch 690: training loss 1.009480 \n",
            "Epoch 691: training loss 1.091796 \n",
            "Epoch 692: training loss 0.580468 \n",
            "Epoch 693: training loss 0.629300 \n",
            "Epoch 694: training loss 1.123272 \n",
            "Epoch 695: training loss 0.914838 \n",
            "Epoch 696: training loss 0.743543 \n",
            "Epoch 697: training loss 0.636504 \n",
            "Epoch 698: training loss 1.290781 \n",
            "Epoch 699: training loss 1.419036 \n",
            "Epoch 700: training loss 1.674522 \n",
            "Epoch 701: training loss 0.430715 \n",
            "Epoch 702: training loss 1.590567 \n",
            "Epoch 703: training loss 0.676821 \n",
            "Epoch 704: training loss 0.961405 \n",
            "Epoch 705: training loss 0.938181 \n",
            "Epoch 706: training loss 0.875807 \n",
            "Epoch 707: training loss 0.632054 \n",
            "Epoch 708: training loss 0.819416 \n",
            "Epoch 709: training loss 0.262327 \n",
            "Epoch 710: training loss 1.451855 \n",
            "Epoch 711: training loss 0.520041 \n",
            "Epoch 712: training loss 0.728036 \n",
            "Epoch 713: training loss 1.774132 \n",
            "Epoch 714: training loss 0.328134 \n",
            "Epoch 715: training loss 0.694598 \n",
            "Epoch 716: training loss 0.757024 \n",
            "Epoch 717: training loss 1.429026 \n",
            "Epoch 718: training loss 0.234250 \n",
            "Epoch 719: training loss 1.026646 \n",
            "Epoch 720: training loss 1.282937 \n",
            "Epoch 721: training loss 0.360428 \n",
            "Epoch 722: training loss 0.571731 \n",
            "Epoch 723: training loss 0.712450 \n",
            "Epoch 724: training loss 0.752455 \n",
            "Epoch 725: training loss 0.553607 \n",
            "Epoch 726: training loss 2.287421 \n",
            "Epoch 727: training loss 0.923964 \n",
            "Epoch 728: training loss 0.960087 \n",
            "Epoch 729: training loss 2.189771 \n",
            "Epoch 730: training loss 0.792331 \n",
            "Epoch 731: training loss 0.814740 \n",
            "Epoch 732: training loss 0.559800 \n",
            "Epoch 733: training loss 1.727811 \n",
            "Epoch 734: training loss 0.927976 \n",
            "Epoch 735: training loss 0.131803 \n",
            "Epoch 736: training loss 2.524812 \n",
            "Epoch 737: training loss 1.326273 \n",
            "Epoch 738: training loss 0.603185 \n",
            "Epoch 739: training loss 0.284886 \n",
            "Epoch 740: training loss 1.836958 \n",
            "Epoch 741: training loss 0.145227 \n",
            "Epoch 742: training loss 0.952211 \n",
            "Epoch 743: training loss 2.095706 \n",
            "Epoch 744: training loss 0.705121 \n",
            "Epoch 745: training loss 1.305664 \n",
            "Epoch 746: training loss 1.130279 \n",
            "Epoch 747: training loss 1.350270 \n",
            "Epoch 748: training loss 0.904404 \n",
            "Epoch 749: training loss 1.502019 \n",
            "Epoch 750: training loss 0.489939 \n",
            "Epoch 751: training loss 0.370649 \n",
            "Epoch 752: training loss 0.711666 \n",
            "Epoch 753: training loss 0.250742 \n",
            "Epoch 754: training loss 0.729081 \n",
            "Epoch 755: training loss 0.476934 \n",
            "Epoch 756: training loss 0.578462 \n",
            "Epoch 757: training loss 1.630759 \n",
            "Epoch 758: training loss 2.554315 \n",
            "Epoch 759: training loss 1.079402 \n",
            "Epoch 760: training loss 0.979134 \n",
            "Epoch 761: training loss 0.547123 \n",
            "Epoch 762: training loss 1.130669 \n",
            "Epoch 763: training loss 0.589962 \n",
            "Epoch 764: training loss 2.016785 \n",
            "Epoch 765: training loss 0.705776 \n",
            "Epoch 766: training loss 0.403581 \n",
            "Epoch 767: training loss 0.343223 \n",
            "Epoch 768: training loss 1.322935 \n",
            "Epoch 769: training loss 1.447009 \n",
            "Epoch 770: training loss 0.791156 \n",
            "Epoch 771: training loss 0.326876 \n",
            "Epoch 772: training loss 0.613540 \n",
            "Epoch 773: training loss 1.646166 \n",
            "Epoch 774: training loss 0.254255 \n",
            "Epoch 775: training loss 0.213807 \n",
            "Epoch 776: training loss 1.705341 \n",
            "Epoch 777: training loss 0.903701 \n",
            "Epoch 778: training loss 0.325374 \n",
            "Epoch 779: training loss 0.919258 \n",
            "Epoch 780: training loss 0.393663 \n",
            "Epoch 781: training loss 0.131723 \n",
            "Epoch 782: training loss 0.839092 \n",
            "Epoch 783: training loss 0.569299 \n",
            "Epoch 784: training loss 1.875303 \n",
            "Epoch 785: training loss 1.960854 \n",
            "Epoch 786: training loss 1.387383 \n",
            "Epoch 787: training loss 0.212705 \n",
            "Epoch 788: training loss 0.563084 \n",
            "Epoch 789: training loss 0.850121 \n",
            "Epoch 790: training loss 1.619218 \n",
            "Epoch 791: training loss 0.364657 \n",
            "Epoch 792: training loss 0.362057 \n",
            "Epoch 793: training loss 0.197098 \n",
            "Epoch 794: training loss 2.442544 \n",
            "Epoch 795: training loss 1.414662 \n",
            "Epoch 796: training loss 1.460769 \n",
            "Epoch 797: training loss 1.076276 \n",
            "Epoch 798: training loss 0.871259 \n",
            "Epoch 799: training loss 1.471763 \n",
            "Epoch 800: training loss 1.097844 \n",
            "Epoch 801: training loss 0.415140 \n",
            "Epoch 802: training loss 1.374942 \n",
            "Epoch 803: training loss 0.331886 \n",
            "Epoch 804: training loss 0.773387 \n",
            "Epoch 805: training loss 0.462256 \n",
            "Epoch 806: training loss 0.781271 \n",
            "Epoch 807: training loss 0.365851 \n",
            "Epoch 808: training loss 1.511490 \n",
            "Epoch 809: training loss 0.127338 \n",
            "Epoch 810: training loss 1.169861 \n",
            "Epoch 811: training loss 0.522960 \n",
            "Epoch 812: training loss 0.385852 \n",
            "Epoch 813: training loss 0.318075 \n",
            "Epoch 814: training loss 0.359285 \n",
            "Epoch 815: training loss 0.605629 \n",
            "Epoch 816: training loss 0.557502 \n",
            "Epoch 817: training loss 0.747668 \n",
            "Epoch 818: training loss 0.832648 \n",
            "Epoch 819: training loss 1.508675 \n",
            "Epoch 820: training loss 1.243531 \n",
            "Epoch 821: training loss 0.280480 \n",
            "Epoch 822: training loss 0.337904 \n",
            "Epoch 823: training loss 2.145584 \n",
            "Epoch 824: training loss 1.103654 \n",
            "Epoch 825: training loss 0.325222 \n",
            "Epoch 826: training loss 1.596465 \n",
            "Epoch 827: training loss 0.736856 \n",
            "Epoch 828: training loss 1.055119 \n",
            "Epoch 829: training loss 0.650698 \n",
            "Epoch 830: training loss 0.583856 \n",
            "Epoch 831: training loss 1.572361 \n",
            "Epoch 832: training loss 1.267849 \n",
            "Epoch 833: training loss 0.441740 \n",
            "Epoch 834: training loss 0.388176 \n",
            "Epoch 835: training loss 0.548188 \n",
            "Epoch 836: training loss 0.491496 \n",
            "Epoch 837: training loss 1.505178 \n",
            "Epoch 838: training loss 0.972196 \n",
            "Epoch 839: training loss 1.691396 \n",
            "Epoch 840: training loss 0.340439 \n",
            "Epoch 841: training loss 1.882111 \n",
            "Epoch 842: training loss 1.345003 \n",
            "Epoch 843: training loss 0.704142 \n",
            "Epoch 844: training loss 1.503114 \n",
            "Epoch 845: training loss 0.925247 \n",
            "Epoch 846: training loss 1.107527 \n",
            "Epoch 847: training loss 1.617862 \n",
            "Epoch 848: training loss 0.772408 \n",
            "Epoch 849: training loss 0.341180 \n",
            "Epoch 850: training loss 1.086750 \n",
            "Epoch 851: training loss 0.242612 \n",
            "Epoch 852: training loss 0.399568 \n",
            "Epoch 853: training loss 0.681291 \n",
            "Epoch 854: training loss 0.423302 \n",
            "Epoch 855: training loss 0.131524 \n",
            "Epoch 856: training loss 0.033468 \n",
            "Epoch 857: training loss 2.919428 \n",
            "Epoch 858: training loss 0.328138 \n",
            "Epoch 859: training loss 1.166399 \n",
            "Epoch 860: training loss 0.943147 \n",
            "Epoch 861: training loss 0.571185 \n",
            "Epoch 862: training loss 0.555925 \n",
            "Epoch 863: training loss 0.546883 \n",
            "Epoch 864: training loss 0.084109 \n",
            "Epoch 865: training loss 1.198678 \n",
            "Epoch 866: training loss 2.210020 \n",
            "Epoch 867: training loss 2.272730 \n",
            "Epoch 868: training loss 0.081215 \n",
            "Epoch 869: training loss 0.489760 \n",
            "Epoch 870: training loss 0.334053 \n",
            "Epoch 871: training loss 1.169369 \n",
            "Epoch 872: training loss 1.392737 \n",
            "Epoch 873: training loss 0.467083 \n",
            "Epoch 874: training loss 0.326617 \n",
            "Epoch 875: training loss 0.278985 \n",
            "Epoch 876: training loss 0.157659 \n",
            "Epoch 877: training loss 1.524732 \n",
            "Epoch 878: training loss 0.090489 \n",
            "Epoch 879: training loss 0.155279 \n",
            "Epoch 880: training loss 0.813032 \n",
            "Epoch 881: training loss 0.096944 \n",
            "Epoch 882: training loss 1.125909 \n",
            "Epoch 883: training loss 0.416168 \n",
            "Epoch 884: training loss 0.751492 \n",
            "Epoch 885: training loss 0.547980 \n",
            "Epoch 886: training loss 1.224192 \n",
            "Epoch 887: training loss 0.214537 \n",
            "Epoch 888: training loss 0.482126 \n",
            "Epoch 889: training loss 0.810565 \n",
            "Epoch 890: training loss 0.141342 \n",
            "Epoch 891: training loss 0.196212 \n",
            "Epoch 892: training loss 1.950446 \n",
            "Epoch 893: training loss 0.337831 \n",
            "Epoch 894: training loss 0.094850 \n",
            "Epoch 895: training loss 0.757009 \n",
            "Epoch 896: training loss 1.256880 \n",
            "Epoch 897: training loss 0.801653 \n",
            "Epoch 898: training loss 0.154310 \n",
            "Epoch 899: training loss 1.616377 \n",
            "Epoch 900: training loss 0.173305 \n",
            "Epoch 901: training loss 1.285259 \n",
            "Epoch 902: training loss 2.031075 \n",
            "Epoch 903: training loss 0.592971 \n",
            "Epoch 904: training loss 2.341091 \n",
            "Epoch 905: training loss 0.134859 \n",
            "Epoch 906: training loss 0.815871 \n",
            "Epoch 907: training loss 0.093881 \n",
            "Epoch 908: training loss 0.129731 \n",
            "Epoch 909: training loss 0.227228 \n",
            "Epoch 910: training loss 0.090878 \n",
            "Epoch 911: training loss 1.265814 \n",
            "Epoch 912: training loss 0.889756 \n",
            "Epoch 913: training loss 0.503680 \n",
            "Epoch 914: training loss 1.792741 \n",
            "Epoch 915: training loss 1.112172 \n",
            "Epoch 916: training loss 0.173956 \n",
            "Epoch 917: training loss 1.650736 \n",
            "Epoch 918: training loss 0.092979 \n",
            "Epoch 919: training loss 0.697348 \n",
            "Epoch 920: training loss 0.973498 \n",
            "Epoch 921: training loss 1.277579 \n",
            "Epoch 922: training loss 0.137056 \n",
            "Epoch 923: training loss 1.274804 \n",
            "Epoch 924: training loss 1.699379 \n",
            "Epoch 925: training loss 0.310146 \n",
            "Epoch 926: training loss 0.471905 \n",
            "Epoch 927: training loss 1.455571 \n",
            "Epoch 928: training loss 1.212569 \n",
            "Epoch 929: training loss 0.118621 \n",
            "Epoch 930: training loss 0.205230 \n",
            "Epoch 931: training loss 0.167851 \n",
            "Epoch 932: training loss 1.783476 \n",
            "Epoch 933: training loss 1.497106 \n",
            "Epoch 934: training loss 0.898005 \n",
            "Epoch 935: training loss 0.127972 \n",
            "Epoch 936: training loss 1.885975 \n",
            "Epoch 937: training loss 0.173416 \n",
            "Epoch 938: training loss 0.523757 \n",
            "Epoch 939: training loss 0.679351 \n",
            "Epoch 940: training loss 0.742576 \n",
            "Epoch 941: training loss 0.103847 \n",
            "Epoch 942: training loss 0.058176 \n",
            "Epoch 943: training loss 1.526721 \n",
            "Epoch 944: training loss 0.136677 \n",
            "Epoch 945: training loss 0.250719 \n",
            "Epoch 946: training loss 0.158756 \n",
            "Epoch 947: training loss 0.840869 \n",
            "Epoch 948: training loss 1.004443 \n",
            "Epoch 949: training loss 0.842573 \n",
            "Epoch 950: training loss 1.581687 \n",
            "Epoch 951: training loss 1.809147 \n",
            "Epoch 952: training loss 0.664149 \n",
            "Epoch 953: training loss 0.203232 \n",
            "Epoch 954: training loss 0.134403 \n",
            "Epoch 955: training loss 1.474289 \n",
            "Epoch 956: training loss 1.200129 \n",
            "Epoch 957: training loss 0.578445 \n",
            "Epoch 958: training loss 0.082440 \n",
            "Epoch 959: training loss 0.139577 \n",
            "Epoch 960: training loss 2.309761 \n",
            "Epoch 961: training loss 0.971564 \n",
            "Epoch 962: training loss 1.263080 \n",
            "Epoch 963: training loss 0.360789 \n",
            "Epoch 964: training loss 0.945321 \n",
            "Epoch 965: training loss 0.258218 \n",
            "Epoch 966: training loss 0.585323 \n",
            "Epoch 967: training loss 0.671512 \n",
            "Epoch 968: training loss 0.215819 \n",
            "Epoch 969: training loss 0.363792 \n",
            "Epoch 970: training loss 0.504420 \n",
            "Epoch 971: training loss 2.410644 \n",
            "Epoch 972: training loss 1.082811 \n",
            "Epoch 973: training loss 0.414201 \n",
            "Epoch 974: training loss 0.080987 \n",
            "Epoch 975: training loss 0.303334 \n",
            "Epoch 976: training loss 0.314103 \n",
            "Epoch 977: training loss 0.178768 \n",
            "Epoch 978: training loss 0.796625 \n",
            "Epoch 979: training loss 0.875530 \n",
            "Epoch 980: training loss 0.787076 \n",
            "Epoch 981: training loss 0.876258 \n",
            "Epoch 982: training loss 1.398602 \n",
            "Epoch 983: training loss 0.482173 \n",
            "Epoch 984: training loss 0.086424 \n",
            "Epoch 985: training loss 1.092677 \n",
            "Epoch 986: training loss 0.301602 \n",
            "Epoch 987: training loss 0.740172 \n",
            "Epoch 988: training loss 0.372346 \n",
            "Epoch 989: training loss 0.478422 \n",
            "Epoch 990: training loss 0.614041 \n",
            "Epoch 991: training loss 0.345162 \n",
            "Epoch 992: training loss 0.075402 \n",
            "Epoch 993: training loss 0.647979 \n",
            "Epoch 994: training loss 2.085042 \n",
            "Epoch 995: training loss 0.302483 \n",
            "Epoch 996: training loss 0.510536 \n",
            "Epoch 997: training loss 1.298757 \n",
            "Epoch 998: training loss 0.220214 \n",
            "Epoch 999: training loss 0.075448 \n",
            "training acc.. 82.1155\n",
            "testing loss.. 82.5\n"
          ]
        }
      ]
    }
  ]
}